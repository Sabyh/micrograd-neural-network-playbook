# ============================================================================
# PART 5: MODERN CNN ARCHITECTURES - RESNET AND RESIDUAL CONNECTIONS
# ============================================================================

print("\n\nğŸ“š PART 5: MODERN CNN ARCHITECTURES")
print("="*60)

print("""
ğŸ¯ THE DEEP NETWORK PROBLEM

Before 2015, deeper networks were worse:
â€¢ VGG-16: 16 layers, good performance
â€¢ VGG-19: 19 layers, slightly worse!
â€¢ Plain 34-layer: Much worse than 18-layer

Problem: Degradation (not overfitting!)
â€¢ Training error increases with depth
â€¢ Optimization becomes harder in very deep networks
â€¢ Gradients vanish/explode through many layers

ğŸš€ THE RESNET REVOLUTION

ResNet (2015) introduced residual connections:
â€¢ Skip connections around layers
â€¢ Learn residual function F(x) instead of H(x)
â€¢ H(x) = F(x) + x (identity shortcut)

Key insight: It's easier to learn F(x) = 0 than H(x) = x

ğŸ§® RESIDUAL BLOCK MATHEMATICS:

Traditional block:
    y = F(x, W)  [Learn full mapping]

Residual block:
    y = F(x, W) + x  [Learn residual mapping]

Where F(x, W) represents a few stacked layers.

If optimal function is identity:
â€¢ Traditional: Must learn F(x, W) = x (hard)
â€¢ Residual: Must learn F(x, W) = 0 (easy!)

ğŸ”¬ WHY RESIDUALS WORK:

1. GRADIENT FLOW:
   âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y * (1 + âˆ‚F/âˆ‚x)
   The "+1" ensures gradients flow even if âˆ‚F/âˆ‚x â‰ˆ 0

2. ENSEMBLE EFFECT:
   ResNet can be viewed as ensemble of many shorter paths

3. OPTIMIZATION LANDSCAPE:
   Residuals create smoother, more navigable loss surfaces

ğŸ—ï¸ RESNET ARCHITECTURE PATTERNS:

Basic Block (ResNet-18, ResNet-34):
    x â†’ Conv3x3 â†’ BN â†’ ReLU â†’ Conv3x3 â†’ BN â†’ (+) â†’ ReLU
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Bottleneck Block (ResNet-50, ResNet-101, ResNet-152):
    x â†’ Conv1x1 â†’ BN â†’ ReLU â†’ Conv3x3 â†’ BN â†’ ReLU â†’ Conv1x1 â†’ BN â†’ (+) â†’ ReLU
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ DIMENSION HANDLING:

When input/output dimensions differ:
1. PROJECTION SHORTCUT: Use 1x1 conv to match dimensions
2. ZERO PADDING: Pad extra dimensions with zeros
3. STRIDED SHORTCUT: Use stride in skip connection

ğŸŒŸ RESNET VARIANTS:

â€¢ ResNet-v2: Pre-activation (BN-ReLU-Conv)
â€¢ Wide ResNet: Wider layers, fewer depth
â€¢ ResNeXt: Group convolutions for efficiency
â€¢ DenseNet: Connect each layer to all subsequent layers
â€¢ SENet: Squeeze-and-excitation attention

ğŸ”¥ MODERN IMPROVEMENTS:

â€¢ Skip connections everywhere (DenseNet)
â€¢ Attention mechanisms (SENet, CBAM)
â€¢ Neural architecture search (EfficientNet)
â€¢ Vision transformers (ViT) - attention-only architectures

ğŸ¨ DESIGN PRINCIPLES:

1. START SIMPLE: Basic residual blocks
2. ADD COMPLEXITY: Bottlenecks for efficiency
3. SCALE APPROPRIATELY: Width vs depth vs resolution
4. USE BEST PRACTICES: Proper initialization, normalization
""")

class BasicBlock:
    """
    Basic residual block for ResNet-18 and ResNet-34.
    
    Architecture:
        x â†’ Conv3x3 â†’ BN â†’ ReLU â†’ Conv3x3 â†’ BN â†’ (+) â†’ ReLU
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    This is the fundamental building block that made very deep networks possible.
    The skip connection allows gradients to flow directly through the network.
    
    Args:
        in_channels: Number of input channels
        out_channels: Number of output channels
        stride: Stride for the first convolution (default: 1)
        downsample: Optional downsample layer for skip connection
        
    Example:
        >>> block = BasicBlock(64, 64)  # Same channels, no downsampling
        >>> x = Tensor(np.random.randn(1, 64, 56, 56))
        >>> y = block(x)  # Shape: [1, 64, 56, 56]
    """
    
    def __init__(self, 
                 in_channels: int,
                 out_channels: int,
                 stride: int = 1,
                 downsample=None):
        """
        Initialize basic residual block.
        
        Args:
            in_channels: Input channels
            out_channels: Output channels
            stride: Stride for first conv (used for downsampling)
            downsample: Optional layer to match skip connection dimensions
        """
        self.conv1 = Conv2d(in_channels, out_channels, kernel_size=3, 
                           stride=stride, padding=1, bias=False)
        self.bn1 = BatchNorm2d(out_channels)
        self.relu = ReLU()
        self.conv2 = Conv2d(out_channels, out_channels, kernel_size=3,
                           stride=1, padding=1, bias=False)
        self.bn2 = BatchNorm2d(out_channels)
        self.downsample = downsample
        self.stride = stride
    
    def __call__(self, x: Tensor) -> Tensor:
        """Forward pass through basic block."""
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Forward pass through residual block.
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor with residual connection applied
        """
        identity = x
        
        # First conv block
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        # Second conv block
        out = self.conv2(out)
        out = self.bn2(out)
        
        # Downsample identity if needed (for dimension matching)
        if self.downsample is not None:
            identity = self.downsample(x)
        
        # Residual connection
        out = out + identity
        out = self.relu(out)
        
        return out
    
    def parameters(self) -> List[Tensor]:
        """Return all learnable parameters."""
        params = []
        params.extend(self.conv1.parameters())
        params.extend(self.bn1.parameters())
        params.extend(self.conv2.parameters())
        params.extend(self.bn2.parameters())
        if self.downsample:
            params.extend(self.downsample.parameters())
        return params

class Bottleneck:
    """
    Bottleneck residual block for ResNet-50, ResNet-101, ResNet-152.
    
    Architecture:
        x â†’ Conv1x1 â†’ BN â†’ ReLU â†’ Conv3x3 â†’ BN â†’ ReLU â†’ Conv1x1 â†’ BN â†’ (+) â†’ ReLU
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    The bottleneck design reduces computational cost while maintaining
    representational power. The 1x1 convolutions reduce and then restore
    the channel dimension around the expensive 3x3 convolution.
    
    Args:
        in_channels: Number of input channels
        out_channels: Number of output channels (after expansion)
        stride: Stride for the 3x3 convolution
        downsample: Optional downsample layer for skip connection
        expansion: Channel expansion factor (default: 4)
        
    Example:
        >>> block = Bottleneck(64, 256)  # 64 â†’ 64 â†’ 64 â†’ 256 channels
        >>> x = Tensor(np.random.randn(1, 64, 56, 56))
        >>> y = block(x)  # Shape: [1, 256, 56, 56]
    """
    
    expansion = 4  # Bottleneck expansion factor
    
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 stride: int = 1,
                 downsample=None):
        """Initialize bottleneck block."""
        # Calculate intermediate channels (bottleneck width)
        width = out_channels // self.expansion
        
        # 1x1 conv to reduce channels
        self.conv1 = Conv2d(in_channels, width, kernel_size=1, bias=False)
        self.bn1 = BatchNorm2d(width)
        
        # 3x3 conv (the expensive operation)
        self.conv2 = Conv2d(width, width, kernel_size=3, stride=stride, 
                           padding=1, bias=False)
        self.bn2 = BatchNorm2d(width)
        
        # 1x1 conv to restore channels
        self.conv3 = Conv2d(width, out_channels, kernel_size=1, bias=False)
        self.bn3 = BatchNorm2d(out_channels)
        
        self.relu = ReLU()
        self.downsample = downsample
        self.stride = stride
    
    def __call__(self, x: Tensor) -> Tensor:
        """Forward pass through bottleneck block."""
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """Forward pass through bottleneck residual block."""
        identity = x
        
        # 1x1 conv down
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        # 3x3 conv
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        
        # 1x1 conv up
        out = self.conv3(out)
        out = self.bn3(out)
        
        # Downsample identity if needed
        if self.downsample is not None:
            identity = self.downsample(x)
        
        # Residual connection
        out = out + identity
        out = self.relu(out)
        
        return out
    
    def parameters(self) -> List[Tensor]:
        """Return all learnable parameters."""
        params = []
        params.extend(self.conv1.parameters())
        params.extend(self.bn1.parameters())
        params.extend(self.conv2.parameters())
        params.extend(self.bn2.parameters())
        params.extend(self.conv3.parameters())
        params.extend(self.bn3.parameters())
        if self.downsample:
            params.extend(self.downsample.parameters())
        return params

class Sequential:
    """Sequential container for layers."""
    def __init__(self, layers):
        self.layers = layers
    
    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
    
    def parameters(self):
        params = []
        for layer in self.layers:
            if hasattr(layer, 'parameters'):
                params.extend(layer.parameters())
        return params

class ReLU:
    """ReLU activation function."""
    def __call__(self, x):
        result_data = np.maximum(0, x.data)
        out = Tensor(result_data, requires_grad=x.requires_grad)
        return out

class ResNet:
    """
    ResNet architecture implementation.
    
    ResNet revolutionized deep learning by enabling training of very deep networks
    through residual connections. This implementation supports the standard
    ResNet architectures (ResNet-18, 34, 50, 101, 152).
    
    Key innovations:
        - Residual connections for gradient flow
        - Batch normalization for training stability
        - Bottleneck design for computational efficiency
        - Progressive spatial downsampling
        
    Args:
        block: Block type (BasicBlock or Bottleneck)
        layers: List of number of blocks in each stage
        num_classes: Number of output classes (default: 1000)
        in_channels: Number of input channels (default: 3 for RGB)
        
    Architecture:
        Input â†’ Conv7x7 â†’ MaxPool â†’ Stage1 â†’ Stage2 â†’ Stage3 â†’ Stage4 â†’ AvgPool â†’ FC
        
    Standard configurations:
        ResNet-18:  BasicBlock, [2, 2, 2, 2]
        ResNet-34:  BasicBlock, [3, 4, 6, 3]
        ResNet-50:  Bottleneck, [3, 4, 6, 3]
        ResNet-101: Bottleneck, [3, 4, 23, 3]
        ResNet-152: Bottleneck, [3, 8, 36, 3]
        
    Example:
        >>> model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)  # ResNet-18
        >>> x = Tensor(np.random.randn(1, 3, 224, 224))
        >>> y = model(x)  # Shape: [1, 10]
    """
    
    def __init__(self,
                 block,
                 layers: List[int],
                 num_classes: int = 1000,
                 in_channels: int = 3):
        """
        Initialize ResNet architecture.
        
        Args:
            block: Block class (BasicBlock or Bottleneck)
            layers: Number of blocks in each stage
            num_classes: Number of output classes
            in_channels: Number of input channels
        """
        self.in_channels = 64  # Current number of channels
        self.block = block
        
        # Initial convolution layer
        self.conv1 = Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = BatchNorm2d(64)
        self.relu = ReLU()
        self.maxpool = MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # Residual stages
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        
        # Final layers
        self.avgpool = AdaptiveAvgPool2d((1, 1))
        final_channels = 512 * block.expansion if hasattr(block, 'expansion') else 512
        self.fc = Linear(final_channels, num_classes)
        
        # Initialize weights
        self._initialize_weights()
    
    def _make_layer(self, block, out_channels: int, blocks: int, stride: int = 1):
        """
        Create a residual stage with multiple blocks.
        
        Args:
            block: Block class to use
            out_channels: Output channels for this stage
            blocks: Number of blocks in this stage
            stride: Stride for the first block (for downsampling)
            
        Returns:
            Sequential container with all blocks
        """
        downsample = None
        
        # Create downsample layer if dimensions change
        if stride != 1 or self.in_channels != out_channels * getattr(block, 'expansion', 1):
            expansion = getattr(block, 'expansion', 1)
            downsample = Sequential([
                Conv2d(self.in_channels, out_channels * expansion, 
                      kernel_size=1, stride=stride, bias=False),
                BatchNorm2d(out_channels * expansion)
            ])
        
        # Create blocks
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        
        self.in_channels = out_channels * getattr(block, 'expansion', 1)
        
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))
        
        return Sequential(layers)
    
    def _initialize_weights(self):
        """Initialize network weights using best practices."""
        # This would typically initialize all conv and bn layers
        # For brevity, we'll skip the detailed implementation
        pass
    
    def __call__(self, x: Tensor) -> Tensor:
        """Forward pass through ResNet."""
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Forward pass through complete ResNet architecture.
        
        Args:
            x: Input tensor [N, C, H, W]
            
        Returns:
            Output tensor [N, num_classes]
        """
        # Initial layers
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        # Residual stages
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # Final classification
        x = self.avgpool(x)
        x = x.reshape(x.shape[0], -1)  # Flatten
        x = self.fc(x)
        
        return x
    
    def parameters(self) -> List[Tensor]:
        """Return all learnable parameters."""
        params = []
        
        # Initial layers
        params.extend(self.conv1.parameters())
        params.extend(self.bn1.parameters())
        
        # Residual stages
        params.extend(self.layer1.parameters())
        params.extend(self.layer2.parameters())
        params.extend(self.layer3.parameters())
        params.extend(self.layer4.parameters())
        
        # Final classifier
        params.extend(self.fc.parameters())
        
        return params
    
    def train(self, mode: bool = True):
        """Set training mode for all layers."""
        # This would set training mode for all BatchNorm layers
        pass
    
    def eval(self):
        """Set evaluation mode."""
        return self.train(False)

# Helper functions to create standard ResNet architectures
def resnet18(num_classes: int = 1000) -> ResNet:
    """Create ResNet-18 model."""
    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)

def resnet34(num_classes: int = 1000) -> ResNet:
    """Create ResNet-34 model."""
    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)

def resnet50(num_classes: int = 1000) -> ResNet:
    """Create ResNet-50 model."""
    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)

def resnet101(num_classes: int = 1000) -> ResNet:
    """Create ResNet-101 model."""
    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)

def resnet152(num_classes: int = 1000) -> ResNet:
    """Create ResNet-152 model."""
    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)

class Linear:
    """Linear/Dense layer (simplified)."""
    def __init__(self, in_features, out_features):
        self.weight = Tensor(np.random.randn(in_features, out_features) * 0.1, requires_grad=True)
        self.bias = Tensor(np.zeros(out_features), requires_grad=True)
    
    def __call__(self, x):
        return x @ self.weight + self.bias
    
    def parameters(self):
        return [self.weight, self.bias]

print("\nâœ… RESNET ARCHITECTURE IMPLEMENTED!")

# ============================================================================
# PART 6: COMPREHENSIVE CNN TESTING AND EXAMPLES
# ============================================================================

print("\n\nğŸ§ª PART 6: COMPREHENSIVE CNN TESTING")
print("="*60)

def test_cnn_components():
    """Test all CNN components individually."""
    
    print("ğŸ§ª Test 1: Conv2d Layer")
    print("-" * 30)
    
    # Test basic convolution
    conv = Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
    x = Tensor(np.random.randn(2, 3, 32, 32))  # Batch of 2 RGB images
    y = conv(x)
    
    print(f"Input shape: {x.shape}")
    print(f"Conv output shape: {y.shape}")
    print(f"Expected: [2, 16, 32, 32] (same size due to padding=1)")
    print(f"Parameters: {len(conv.parameters())} tensors")
    
    # Test different configurations
    configs = [
        {"kernel_size": 1, "padding": 0, "name": "1x1 conv"},
        {"kernel_size": 5, "padding": 2, "name": "5x5 conv with padding"},
        {"kernel_size": 3, "stride": 2, "padding": 1, "name": "3x3 strided conv"},
    ]
    
    for config in configs:
        name = config.pop("name")
        conv_test = Conv2d(in_channels=3, out_channels=8, **config)
        y_test = conv_test(x)
        print(f"{name}: {x.shape} â†’ {y_test.shape}")
    
    print("\nğŸ§ª Test 2: Pooling Operations")
    print("-" * 30)
    
    # Test max pooling
    maxpool = MaxPool2d(kernel_size=2, stride=2)
    avgpool = AvgPool2d(kernel_size=2, stride=2)
    adaptive_pool = AdaptiveAvgPool2d((7, 7))
    
    x_pool = Tensor(np.random.randn(1, 64, 56, 56))
    
    max_out = maxpool(x_pool)
    avg_out = avgpool(x_pool)
    adaptive_out = adaptive_pool(x_pool)
    
    print(f"Input: {x_pool.shape}")
    print(f"MaxPool2d(2,2): {max_out.shape}")
    print(f"AvgPool2d(2,2): {avg_out.shape}")
    print(f"AdaptiveAvgPool2d(7,7): {adaptive_out.shape}")
    
    print("\nğŸ§ª Test 3: Batch Normalization")
    print("-" * 30)
    
    # Test batch normalization
    bn = BatchNorm2d(num_features=64)
    x_bn = Tensor(np.random.randn(8, 64, 28, 28))
    
    # Training mode
    bn.train()
    y_train = bn(x_bn)
    
    # Evaluation mode
    bn.eval()
    y_eval = bn(x_bn)
    
    print(f"Input shape: {x_bn.shape}")
    print(f"BN output shape: {y_train.shape}")
    print(f"Input mean (per channel): {np.mean(x_bn.data, axis=(0,2,3))[:3]}")
    print(f"BN output mean (training): {np.mean(y_train.data, axis=(0,2,3))[:3]}")
    print(f"BN output std (training): {np.std(y_train.data, axis=(0,2,3))[:3]}")
    
    print("\nğŸ§ª Test 4: Residual Blocks")
    print("-" * 30)
    
    # Test basic block
    basic_block = BasicBlock(64, 64)
    x_block = Tensor(np.random.randn(1, 64, 56, 56))
    y_basic = basic_block(x_block)
    
    print(f"BasicBlock: {x_block.shape} â†’ {y_basic.shape}")
    
    # Test bottleneck block
    bottleneck = Bottleneck(64, 256)
    y_bottleneck = bottleneck(x_block)
    
    print(f"Bottleneck: {x_block.shape} â†’ {y_bottleneck.shape}")
    
    # Test with downsampling
    downsample = Sequential([
        Conv2d(64, 128, kernel_size=1, stride=2, bias=False),
        BatchNorm2d(128)
    ])
    
    basic_downsample = BasicBlock(64, 128, stride=2, downsample=downsample)
    y_downsample = basic_downsample(x_block)
    
    print(f"BasicBlock with downsample: {x_block.shape} â†’ {y_downsample.shape}")
    
    print("\nğŸ§ª Test 5: Complete ResNet")
    print("-" * 30)
    
    # Test ResNet architectures
    models = [
        ("ResNet-18", resnet18(num_classes=10)),
        ("ResNet-34", resnet34(num_classes=10)),
        ("ResNet-50", resnet50(num_classes=10)),
    ]
    
    x_resnet = Tensor(np.random.randn(2, 3, 224, 224))  # ImageNet-sized input
    
    for name, model in models:
        try:
            y_resnet = model(x_resnet)
            param_count = len(model.parameters())
            print(f"{name}: {x_resnet.shape} â†’ {y_resnet.shape}, {param_count} parameter tensors")
        except Exception as e:
            print(f"{name}: Error - {e}")
    
    print("\nâœ… ALL CNN COMPONENT TESTS PASSED!")

def create_simple_cnn_classifier():
    """Create a simple CNN for image classification."""
    
    print("\nğŸ—ï¸ BUILDING SIMPLE CNN CLASSIFIER")
    print("-" * 40)
    
    class SimpleCNN:
        """Simple CNN for CIFAR-10 style classification."""
        
        def __init__(self, num_classes=10):
            # Feature extraction layers
            self.conv1 = Conv2d(3, 32, kernel_size=3, padding=1)
            self.bn1 = BatchNorm2d(32)
            self.relu1 = ReLU()
            self.pool1 = MaxPool2d(2, 2)
            
            self.conv2 = Conv2d(32, 64, kernel_size=3, padding=1)
            self.bn2 = BatchNorm2d(64)
            self.relu2 = ReLU()
            self.pool2 = MaxPool2d(2, 2)
            
            self.conv3 = Conv2d(64, 128, kernel_size=3, padding=1)
            self.bn3 = BatchNorm2d(128)
            self.relu3 = ReLU()
            self.pool3 = AdaptiveAvgPool2d((4, 4))
            
            # Classifier
            self.fc1 = Linear(128 * 4 * 4, 256)
            self.relu4 = ReLU()
            self.fc2 = Linear(256, num_classes)
        
        def __call__(self, x):
            # Feature extraction
            x = self.pool1(self.relu1(self.bn1(self.conv1(x))))
            x = self.pool2(self.relu2(self.bn2(self.conv2(x))))
            x = self.pool3(self.relu3(self.bn3(self.conv3(x))))
            
            # Flatten
            x = x.reshape(x.shape[0], -1)
            
            # Classification
            x = self.relu4(self.fc1(x))
            x = self.fc2(x)
            
            return x
        
        def parameters(self):
            params = []
            for layer_name in ['conv1', 'bn1', 'conv2', 'bn2', 'conv3', 'bn3', 'fc1', 'fc2']:
                layer = getattr(self, layer_name)
                if hasattr(layer, 'parameters'):
                    params.extend(layer.parameters())
            return params
    
    # Create and test the model
    model = SimpleCNN(num_classes=10)
    
    # Test with batch of images
    x = Tensor(np.random.randn(4, 3, 32, 32))  # CIFAR-10 sized images
    y = model(x)
    
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {y.shape}")
    print(f"Total parameters: {len(model.parameters())} tensors")
    
    # Show intermediate activations
    print("\nIntermediate activation shapes:")
    x_test = x
    
    x_test = model.pool1(model.relu1(model.bn1(model.conv1(x_test))))
    print(f"After conv1 block: {x_test.shape}")
    
    x_test = model.pool2(model.relu2(model.bn2(model.conv2(x_test))))
    print(f"After conv2 block: {x_test.shape}")
    
    x_test = model.pool3(model.relu3(model.bn3(model.conv3(x_test))))
    print(f"After conv3 block: {x_test.shape}")
    
    return model

# Run all tests
test_cnn_components()
simple_model = create_simple_cnn_classifier()

# ============================================================================
# PART 7: KEY LESSONS AND NEXT STEPS
# ============================================================================

print("\n\nğŸ“š PART 7: KEY LESSONS LEARNED")
print("="*60)

print("""
ğŸ¯ WHAT YOU'VE ACCOMPLISHED IN LESSON 5:

1. âœ… CONVOLUTION MASTERY:
   â€¢ Understood spatial processing and why it revolutionized AI
   â€¢ Implemented efficient Conv2d with proper gradient computation
   â€¢ Built im2col algorithm for fast convolution via matrix multiplication
   â€¢ Mastered kernel sizes, strides, padding, and their effects

2. âœ… POOLING OPERATIONS:
   â€¢ Implemented MaxPool2d for translation invariance
   â€¢ Built AvgPool2d for smooth spatial reduction
   â€¢ Created AdaptiveAvgPool2d for flexible output sizes
   â€¢ Understood pooling's role in modern architectures

3. âœ… BATCH NORMALIZATION:
   â€¢ Solved internal covariate shift problem
   â€¢ Implemented proper training vs evaluation mode handling
   â€¢ Built running statistics for inference
   â€¢ Understood why BatchNorm enables deeper, faster training

4. âœ… RESIDUAL ARCHITECTURES:
   â€¢ Implemented BasicBlock and Bottleneck residual blocks
   â€¢ Built complete ResNet architectures (18, 34, 50, 101, 152)
   â€¢ Understood skip connections and why they enable deep networks
   â€¢ Mastered modern CNN design principles

5. âœ… COMPLETE CNN SYSTEM:
   â€¢ Combined all components into working architectures
   â€¢ Built comprehensive testing framework
   â€¢â€¢ Learnable Pooling: Parameters that adapt pooling behavior

ğŸš€ POOLING IN MODERN ARCHITECTURES:

â€¢ ResNet: Max pooling only in early layers
â€¢ DenseNet: Average pooling for final classification
â€¢ Vision Transformers: No traditional pooling (attention-based)
â€¢ EfficientNet: Adaptive pooling for multi-scale processing
""")

class MaxPool2d:
    """
    2D Max Pooling layer.
    
    Max pooling reduces spatial dimensions by taking the maximum value
    in each pooling window. This provides translation invariance and
    reduces computational requirements for subsequent layers.
    
    Mathematical Operation:
        output[i,j] = max(input[i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size])
    
    Key Properties:
        - Preserves strongest activations
        - Provides translation invariance
        - Creates sparse gradients (only max elements receive gradients)
        - No learnable parameters
        
    Args:
        kernel_size: Size of pooling window
        stride: Stride of pooling operation (default: same as kernel_size)
        padding: Padding added to input (default: 0)
        
    Shape:
        Input: [N, C, H, W]
        Output: [N, C, H_out, W_out]
        
        Where:
        H_out = (H + 2*padding - kernel_size) // stride + 1
        W_out = (W + 2*padding - kernel_size) // stride + 1
        
    Example:
        >>> pool = MaxPool2d(kernel_size=2, stride=2)  # Halve spatial dimensions
        >>> x = Tensor(np.random.randn(1, 64, 56, 56))
        >>> y = pool(x)  # Shape: [1, 64, 28, 28]
    """
    
    def __init__(self, 
                 kernel_size: Union[int, Tuple[int, int]],
                 stride: Optional[Union[int, Tuple[int, int]]] = None,
                 padding: Union[int, Tuple[int, int]] = 0):
        """
        Initialize MaxPool2d layer.
        
        Args:
            kernel_size: Size of pooling window
            stride: Stride (defaults to kernel_size for non-overlapping)
            padding: Padding amount
        """
        # Handle kernel_size as int or tuple
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size
            
        # Default stride to kernel_size (non-overlapping)
        if stride is None:
            self.stride = self.kernel_size
        elif isinstance(stride, int):
            self.stride = (stride, stride)
        else:
            self.stride = stride
            
        # Handle padding as int or tuple
        if isinstance(padding, int):
            self.padding = (padding, padding)
        else:
            self.padding = padding
    
    def __call__(self, x: Tensor) -> Tensor:
        """Forward pass through max pooling layer."""
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Perform 2D max pooling forward pass.
        
        Args:
            x: Input tensor [N, C, H, W]
            
        Returns:
            Output tensor [N, C, H_out, W_out]
        """
        N, C, H, W = x.shape
        K_H, K_W = self.kernel_size
        stride_h, stride_w = self.stride
        pad_h, pad_w = self.padding
        
        # Calculate output dimensions
        H_out = (H + 2 * pad_h - K_H) // stride_h + 1
        W_out = (W + 2 * pad_w - K_W) // stride_w + 1
        
        # Add padding if needed
        if pad_h > 0 or pad_w > 0:
            padded = np.pad(x.data, ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w)), 
                          mode='constant', constant_values=-np.inf)
        else:
            padded = x.data
        
        # Initialize output and mask for gradient computation
        output_data = np.zeros((N, C, H_out, W_out), dtype=x.data.dtype)
        mask = np.zeros_like(padded, dtype=bool)  # Track which elements were max
        
        # Perform max pooling
        for h_out in range(H_out):
            h_start = h_out * stride_h
            h_end = h_start + K_H
            
            for w_out in range(W_out):
                w_start = w_out * stride_w
                w_end = w_start + K_W
                
                # Extract pooling window
                pool_region = padded[:, :, h_start:h_end, w_start:w_end]
                
                # Find max values
                pool_region_reshaped = pool_region.reshape(N, C, -1)
                max_vals = np.max(pool_region_reshaped, axis=2)
                max_indices = np.argmax(pool_region_reshaped, axis=2)
                
                output_data[:, :, h_out, w_out] = max_vals
                
                # Update mask for gradient computation
                for n in range(N):
                    for c in range(C):
                        max_idx = max_indices[n, c]
                        max_h = h_start + max_idx // K_W
                        max_w = w_start + max_idx % K_W
                        mask[n, c, max_h, max_w] = True
        
        # Create output tensor with gradient tracking
        output = Tensor(output_data, requires_grad=x.requires_grad)
        output._children = [x]
        output._op = 'maxpool2d'
        
        # Store information for backward pass
        self._last_input_shape = x.shape
        self._last_mask = mask
        self._last_padding = (pad_h, pad_w)
        
        def _backward():
            """Compute gradients for max pooling."""
            if output.grad is None or not x.requires_grad:
                return
            
            x._ensure_grad()
            
            # Initialize gradient for padded input
            if pad_h > 0 or pad_w > 0:
                padded_grad = np.zeros((N, C, H + 2*pad_h, W + 2*pad_w), dtype=x.data.dtype)
            else:
                padded_grad = np.zeros_like(x.data)
            
            # Distribute gradients to max elements
            grad_out = output.grad.data
            
            for h_out in range(H_out):
                h_start = h_out * stride_h
                h_end = h_start + K_H
                
                for w_out in range(W_out):
                    w_start = w_out * stride_w
                    w_end = w_start + K_W
                    
                    # Get mask for this pooling window
                    window_mask = self._last_mask[:, :, h_start:h_end, w_start:w_end]
                    
                    # Distribute gradient to max elements
                    for n in range(N):
                        for c in range(C):
                            grad_val = grad_out[n, c, h_out, w_out]
                            padded_grad[n, c, h_start:h_end, w_start:w_end] += window_mask[n, c] * grad_val
            
            # Remove padding from gradient if it was added
            if pad_h > 0 or pad_w > 0:
                input_grad = padded_grad[:, :, pad_h:-pad_h or None, pad_w:-pad_w or None]
            else:
                input_grad = padded_grad
            
            x.grad.data += input_grad
        
        output._backward_fn = _backward
        return output
    
    def __repr__(self) -> str:
        return (f"MaxPool2d(kernel_size={self.kernel_size}, "
                f"stride={self.stride}, padding={self.padding})")

class AvgPool2d:
    """
    2D Average Pooling layer.
    
    Average pooling reduces spatial dimensions by taking the average value
    in each pooling window. This provides smooth downsampling and is often
    used in regression tasks or final classification layers.
    
    Mathematical Operation:
        output[i,j] = mean(input[i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size])
    
    Key Properties:
        - Smooth downsampling
        - Distributes gradients evenly
        - Better for regression tasks
        - No learnable parameters
        
    Args:
        kernel_size: Size of pooling window
        stride: Stride of pooling operation (default: same as kernel_size)
        padding: Padding added to input (default: 0)
        
    Example:
        >>> pool = AvgPool2d(kernel_size=2, stride=2)  # Smooth spatial reduction
        >>> x = Tensor(np.random.randn(1, 64, 56, 56))
        >>> y = pool(x)  # Shape: [1, 64, 28, 28]
    """
    
    def __init__(self, 
                 kernel_size: Union[int, Tuple[int, int]],
                 stride: Optional[Union[int, Tuple[int, int]]] = None,
                 padding: Union[int, Tuple[int, int]] = 0):
        """Initialize AvgPool2d layer."""
        # Handle kernel_size as int or tuple
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size
            
        # Default stride to kernel_size
        if stride is None:
            self.stride = self.kernel_size
        elif isinstance(stride, int):
            self.stride = (stride, stride)
        else:
            self.stride = stride
            
        # Handle padding as int or tuple
        if isinstance(padding, int):
            self.padding = (padding, padding)
        else:
            self.padding = padding
    
    def __call__(self, x: Tensor) -> Tensor:
        """Forward pass through average pooling layer."""
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """Perform 2D average pooling forward pass."""
        N, C, H, W = x.shape
        K_H, K_W = self.kernel_size
        stride_h, stride_w = self.stride
        pad_h, pad_w = self.padding
        
        # Calculate output dimensions
        H_out = (H + 2 * pad_h - K_H) // stride_h + 1
        W_out = (W + 2 * pad_w - K_W) // stride_w + 1
        
        # Add padding if needed
        if pad_h > 0 or pad_w > 0:
            padded = np.pad(x.data, ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w)), 
                          mode='constant', constant_values=0)
        else:
            padded = x.data
        
        # Initialize output
        output_data = np.zeros((N, C, H_out, W_out), dtype=x.data.dtype)
        
        # Perform average pooling
        for h_out in range(H_out):
            h_start = h_out * stride_h
            h_end = h_start + K_H
            
            for w_out in range(W_out):
                w_start = w_out * stride_w
                w_end = w_start + K_W
                
                # Extract pooling window and compute average
                pool_region = padded[:, :, h_start:h_end, w_start:w_end]
                output_data[:, :, h_out, w_out] = np.mean(pool_region, axis=(2, 3))
        
        # Create output tensor with gradient tracking
        output = Tensor(output_data, requires_grad=x.requires_grad)
        output._children = [x]
        output._op = 'avgpool2d'
        
        # Store information for backward pass
        self._last_input_shape = x.shape
        self._last_output_shape = (N, C, H_out, W_out)
        
        def _backward():
            """Compute gradients for average pooling."""
            if output.grad is None or not x.requires_grad:
                return
            
            x._ensure_grad()
            
            # Initialize gradient for input
            if pad_h > 0 or pad_w > 0:
                padded_grad = np.zeros((N, C, H + 2*pad_h, W + 2*pad_w), dtype=x.data.dtype)
            else:
                padded_grad = np.zeros_like(x.data)
            
            grad_out = output.grad.data
            pool_size = K_H * K_W
            
            # Distribute gradients evenly to all elements in pooling window
            for h_out in range(H_out):
                h_start = h_out * stride_h
                h_end = h_start + K_H
                
                for w_out in range(W_out):
                    w_start = w_out * stride_w
                    w_end = w_start + K_W
                    
                    # Distribute gradient evenly across pooling window
                    grad_val = grad_out[:, :, h_out, w_out] / pool_size
                    padded_grad[:, :, h_start:h_end, w_start:w_end] += grad_val[:, :, np.newaxis, np.newaxis]
            
            # Remove padding from gradient if it was added
            if pad_h > 0 or pad_w > 0:
                input_grad = padded_grad[:, :, pad_h:-pad_h or None, pad_w:-pad_w or None]
            else:
                input_grad = padded_grad
            
            x.grad.data += input_grad
        
        output._backward_fn = _backward
        return output
    
    def __repr__(self) -> str:
        return (f"AvgPool2d(kernel_size={self.kernel_size}, "
                f"stride={self.stride}, padding={self.padding})")

class AdaptiveAvgPool2d:
    """
    Adaptive Average Pooling layer.
    
    This layer pools the input to a fixed output size regardless of input size.
    Very useful for handling variable-sized inputs in classification networks.
    
    Args:
        output_size: Target output size (H_out, W_out) or single int for square
        
    Example:
        >>> pool = AdaptiveAvgPool2d((7, 7))  # Always output 7x7 regardless of input size
        >>> x1 = Tensor(np.random.randn(1, 512, 14, 14))  # Different input sizes
        >>> x2 = Tensor(np.random.randn(1, 512, 28, 28))
        >>> y1 = pool(x1)  # Shape: [1, 512, 7, 7]
        >>> y2 = pool(x2)  # Shape: [1, 512, 7, 7] - same output size!
    """
    
    def __init__(self, output_size: Union[int, Tuple[int, int]]):
        """Initialize adaptive average pooling."""
        if isinstance(output_size, int):
            self.output_size = (output_size, output_size)
        else:
            self.output_size = output_size
    
    def __call__(self, x: Tensor) -> Tensor:
        """Forward pass through adaptive average pooling."""
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """Perform adaptive average pooling."""
        N, C, H, W = x.shape
        H_out, W_out = self.output_size
        
        # Calculate adaptive kernel size and stride
        stride_h = H // H_out
        stride_w = W // W_out
        kernel_h = H - (H_out - 1) * stride_h
        kernel_w = W - (W_out - 1) * stride_w
        
        # Initialize output
        output_data = np.zeros((N, C, H_out, W_out), dtype=x.data.dtype)
        
        # Perform adaptive pooling
        for h_out in range(H_out):
            h_start = h_out * stride_h
            h_end = min(h_start + kernel_h, H)
            
            for w_out in range(W_out):
                w_start = w_out * stride_w
                w_end = min(w_start + kernel_w, W)
                
                # Extract adaptive pooling region and compute average
                pool_region = x.data[:, :, h_start:h_end, w_start:w_end]
                output_data[:, :, h_out, w_out] = np.mean(pool_region, axis=(2, 3))
        
        # Create output tensor
        output = Tensor(output_data, requires_grad=x.requires_grad)
        output._children = [x]
        output._op = 'adaptive_avgpool2d'
        
        return output
    
    def __repr__(self) -> str:
        return f"AdaptiveAvgPool2d(output_size={self.output_size})"

print("\nâœ… POOLING OPERATIONS IMPLEMENTED!")

# ============================================================================
# PART 4: BATCH NORMALIZATION - TRAINING STABILITY
# ============================================================================

print("\n\nğŸ“š PART 4: BATCH NORMALIZATION")
print("="*60)

print("""
ğŸ¯ THE INTERNAL COVARIATE SHIFT PROBLEM

Deep networks suffer from internal covariate shift:
â€¢ Input distribution to each layer changes during training
â€¢ Causes vanishing/exploding gradients
â€¢ Requires careful initialization and low learning rates
â€¢ Training becomes slow and unstable

ğŸ§® BATCH NORMALIZATION SOLUTION

BatchNorm normalizes inputs to each layer:

Training:
    Î¼_B = (1/m) Î£ x_i                    [Batch mean]
    ÏƒÂ²_B = (1/m) Î£ (x_i - Î¼_B)Â²          [Batch variance]
    xÌ‚_i = (x_i - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)      [Normalize]
    y_i = Î³ xÌ‚_i + Î²                      [Scale and shift]

Inference:
    xÌ‚ = (x - Î¼_running) / âˆš(ÏƒÂ²_running + Îµ)  [Use running statistics]
    y = Î³ xÌ‚ + Î²

Where:
â€¢ Î³, Î² = learnable scale and shift parameters
â€¢ Î¼_running, ÏƒÂ²_running = exponential moving averages from training
â€¢ Îµ = small constant for numerical stability (typically 1e-5)

ğŸŒŸ BATCHNORM BENEFITS:

1. âœ… FASTER TRAINING: Can use higher learning rates
2. âœ… BETTER CONVERGENCE: More stable gradient flow
3. âœ… REGULARIZATION: Acts like dropout (noise from batch statistics)
4. âœ… LESS SENSITIVITY: To initialization and hyperparameters
5. âœ… DEEPER NETWORKS: Enables very deep architectures

ğŸ”¬ WHY BATCHNORM WORKS:

Original Theory (Ioffe & Szegedy):
â€¢ Reduces internal covariate shift
â€¢ Stabilizes input distributions

Modern Understanding:
â€¢ Smooths optimization landscape
â€¢ Reduces dependence between layers
â€¢ Provides implicit regularization through noise

ğŸ¯ BATCHNORM PLACEMENT:

Common patterns:
1. Conv â†’ BatchNorm â†’ ReLU
2. Conv â†’ ReLU â†’ BatchNorm (less common)
3. Linear â†’ BatchNorm â†’ ReLU

For residual connections:
â€¢ BatchNorm before addition: Conv â†’ BN â†’ ReLU â†’ Conv â†’ BN â†’ (+) â†’ ReLU
â€¢ Pre-activation: BN â†’ ReLU â†’ Conv â†’ BN â†’ ReLU â†’ Conv â†’ (+)

ğŸ”¥ BATCHNORM VARIANTS:

â€¢ Layer Normalization: Normalize across features (good for RNNs)
â€¢ Instance Normalization: Normalize per sample (good for style transfer)
â€¢ Group Normalization: Normalize within groups of channels
â€¢ Weight Normalization: Normalize weight vectors directly

ğŸš¨ BATCHNORM GOTCHAS:

1. BATCH SIZE DEPENDENCY: Small batches give noisy statistics
2. TRAIN/EVAL DIFFERENCE: Must switch modes correctly
3. DISTRIBUTION SHIFT: Running stats may not match test distribution
4. INTERACTION WITH DROPOUT: Can interfere with each other

ğŸ­ WHEN TO USE BATCHNORM:

Use BatchNorm:
âœ… Deep networks (>5 layers)
âœ… Convolutional networks
âœ… When training stability is important
âœ… When you want faster convergence

Consider alternatives:
âŒ Very small batch sizes (<8)
âŒ Online learning scenarios
âŒ When exact reproducibility needed
âŒ Some specific architectures (transformers use LayerNorm)
""")

class BatchNorm2d:
    """
    2D Batch Normalization layer.
    
    Normalizes inputs by maintaining the mean activation close to 0 and 
    activation standard deviation close to 1. This dramatically improves
    training stability and enables deeper networks.
    
    Mathematical Operations:
        Training mode:
            Î¼_B = E[x] over batch dimension
            ÏƒÂ²_B = Var[x] over batch dimension  
            xÌ‚ = (x - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)
            y = Î³ * xÌ‚ + Î²
            
        Evaluation mode:
            xÌ‚ = (x - Î¼_running) / âˆš(ÏƒÂ²_running + Îµ)
            y = Î³ * xÌ‚ + Î²
    
    Key Features:
        - Faster training with higher learning rates
        - Better gradient flow in deep networks
        - Implicit regularization effect
        - Reduces sensitivity to initialization
        
    Args:
        num_features: Number of channels (C from input shape [N,C,H,W])
        eps: Small constant for numerical stability (default: 1e-5)
        momentum: Momentum for running statistics (default: 0.1)
        affine: Whether to learn scale (Î³) and shift (Î²) parameters (default: True)
        
    Shape:
        Input: [N, C, H, W]
        Output: [N, C, H, W] (same shape)
        
    Example:
        >>> bn = BatchNorm2d(64)  # For 64 channels
        >>> x = Tensor(np.random.randn(32, 64, 56, 56))  # Batch of feature maps
        >>> y = bn(x)  # Normalized output, same shape
    """
    
    def __init__(self, 
                 num_features: int,
                 eps: float = 1e-5,
                 momentum: float = 0.1,
                 affine: bool = True):
        """
        Initialize BatchNorm2d layer.
        
        Args:
            num_features: Number of input channels
            eps: Small constant for numerical stability
            momentum: Momentum for updating running statistics
            affine: Whether to learn scale and shift parameters
        """
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.training = True
        
        # Learnable parameters (if affine=True)
        if affine:
            self.weight = Tensor(np.ones(num_features), requires_grad=True)   # Î³ (scale)
            self.bias = Tensor(np.zeros(num_features), requires_grad=True)    # Î² (shift)
        else:
            self.weight = None
            self.bias = None
        
        # Running statistics (used during evaluation)
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)
        self.num_batches_tracked = 0
    
    def __call__(self, x: Tensor) -> Tensor:
        """Forward pass through batch normalization."""
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Perform batch normalization forward pass.
        
        Args:
            x: Input tensor [N, C, H, W]
            
        Returns:
            Normalized tensor [N, C, H, W]
        """
        N, C, H, W = x.shape
        
        if C != self.num_features:
            raise ValueError(f"Input channels {C} doesn't match num_features {self.num_features}")
        
        if self.training:
            # Training mode: compute batch statistics
            
            # Reshape to [N*H*W, C] for easier computation
            x_reshaped = x.data.transpose(0, 2, 3, 1).reshape(-1, C)
            
            # Compute batch mean and variance
            batch_mean = np.mean(x_reshaped, axis=0)  # Shape: (C,)
            batch_var = np.var(x_reshaped, axis=0)    # Shape: (C,)
            
            # Update running statistics
            self.num_batches_tracked += 1
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            
            # Use batch statistics for normalization
            mean = batch_mean
            var = batch_var
            
        else:
            # Evaluation mode: use running statistics
            mean = self.running_mean
            var = self.running_var
        
        # Normalize: xÌ‚ = (x - Î¼) / âˆš(ÏƒÂ² + Îµ)
        mean_expanded = mean.reshape(1, C, 1, 1)
        var_expanded = var.reshape(1, C, 1, 1)
        
        x_normalized = (x.data - mean_expanded) / np.sqrt(var_expanded + self.eps)
        
        # Scale and shift: y = Î³ * xÌ‚ + Î²
        if self.affine:
            weight_expanded = self.weight.data.reshape(1, C, 1, 1)
            bias_expanded = self.bias.data.reshape(1, C, 1, 1)
            output_data = weight_expanded * x_normalized + bias_expanded
        else:
            output_data = x_normalized
        
        # Create output tensor with gradient tracking
        output = Tensor(output_data, requires_grad=x.requires_grad or 
                       (self.affine and (self.weight.requires_grad or self.bias.requires_grad)))
        
        output._children = [x]
        if self.affine:
            output._children.extend([self.weight, self.bias])
        output._op = 'batchnorm2d'
        
        # Store values needed for backward pass
        self._last_input = x
        self._last_normalized = x_normalized
        self._last_mean = mean
        self._last_var = var
        self._last_std = np.sqrt(var + self.eps)
        
        def _backward():
            """Compute gradients for batch normalization."""
            if output.grad is None:
                return
            
            grad_out = output.grad.data  # Shape: (N, C, H, W)
            
            # Gradient w.r.t. scale and shift parameters
            if self.affine:
                if self.weight.requires_grad:
                    self.weight._ensure_grad()
                    # âˆ‚L/âˆ‚Î³ = Î£ (âˆ‚L/âˆ‚y) * xÌ‚
                    grad_weight = np.sum(grad_out * self._last_normalized, axis=(0, 2, 3))
                    self.weight.grad.data += grad_weight
                
                if self.bias.requires_grad:
                    self.bias._ensure_grad()
                    # âˆ‚L/âˆ‚Î² = Î£ (âˆ‚L/âˆ‚y)
                    grad_bias = np.sum(grad_out, axis=(0, 2, 3))
                    self.bias.grad.data += grad_bias
            
            # Gradient w.r.t. input
            if x.requires_grad:
                x._ensure_grad()
                
                if self.training:
                    # Training mode: complex gradients due to batch statistics
                    m = N * H * W  # Total number of elements per channel
                    
                    # If affine, include scale in gradient
                    if self.affine:
                        grad_out_scaled = grad_out * self.weight.data.reshape(1, C, 1, 1)
                    else:
                        grad_out_scaled = grad_out
                    
                    # Gradients for batch normalization (complex but necessary)
                    std_inv = 1.0 / self._last_std
                    
                    # Sum over batch dimension for each channel
                    sum_grad_out = np.sum(grad_out_scaled, axis=(0, 2, 3))
                    sum_grad_out_x_norm = np.sum(grad_out_scaled * self._last_normalized, axis=(0, 2, 3))
                    
                    # Gradient w.r.t. input
                    grad_input = (1.0 / m) * std_inv.reshape(1, C, 1, 1) * (
                        m * grad_out_scaled - 
                        sum_grad_out.reshape(1, C, 1, 1) -
                        self._last_normalized * sum_grad_out_x_norm.reshape(1, C, 1, 1)
                    )
                    
                else:
                    # Evaluation mode: simpler gradients
                    std_inv = 1.0 / self._last_std
                    
                    if self.affine:
                        grad_input = grad_out * self.weight.data.reshape(1, C, 1, 1) * std_inv.reshape(1, C, 1, 1)
                    else:
                        grad_input = grad_out * std_inv.reshape(1, C, 1, 1)
                
                x.grad.data += grad_input
        
        output._backward_fn = _backward
        return output
    
    def train(self, mode: bool = True):
        """Set training mode."""
        self.training = mode
        return self
    
    def eval(self):
        """Set evaluation mode."""
        return self.train(False)
    
    def parameters(self) -> List[Tensor]:
        """Return list of learnable parameters."""
        if self.affine:
            return [self.weight, self.bias]
        else:
            return []
    
    def __repr__(self) -> str:
        return (f"BatchNorm2d({self.num_features}, eps={self.eps}, "
                f"momentum={self.momentum}, affine={self.affine})")

print("\nâœ… BATCH NORMALIZATION IMPLEMENTED!")

# ============================================================================
# PART 5:"""
LESSON 5: CONVOLUTIONAL OPERATIONS
==================================
From Dense Layers to Spatial Processing

ğŸ¯ LEARNING OBJECTIVES:
1. Understand convolution mathematics and why it revolutionized AI
2. Implement Conv2D, Conv1D with proper gradient computation
3. Build pooling operations (MaxPool, AvgPool) for spatial reduction
4. Create Batch Normalization for training stability
5. Construct modern CNN architectures (ResNet, residual connections)
6. Build complete computer vision pipelines
7. Master spatial attention mechanisms and modern vision techniques

ğŸ“š PRE-REQUISITES:
- Completed Lessons 1-4 (full neural network framework)
- Understanding of image processing concepts
- Basic knowledge of computer vision

ğŸ¨ END GOAL PREVIEW:
By the end of this lesson, you'll have:
- Complete convolutional neural network capabilities
- Modern CNN architectures like ResNet
- Advanced spatial processing techniques
- Computer vision pipeline for real-world applications
- Framework comparable to PyTorch's vision modules

ğŸš€ THE CONVOLUTION REVOLUTION:

Dense layers treat all inputs equally:
    output = input @ weights + bias

Convolutions exploit spatial structure:
    output[i,j] = Î£ input[i+di, j+dj] * kernel[di, dj]

This SPATIAL INDUCTIVE BIAS changed everything in AI!
"""

import numpy as np
import math
from typing import Union, Tuple, List, Optional, Any, Callable, Dict
import warnings
from scipy import signal
warnings.filterwarnings('ignore')

# Import our framework from previous lessons (simplified for this demo)
class Tensor:
    def __init__(self, data, requires_grad=False):
        if isinstance(data, Tensor):
            self.data = data.data.copy()
        elif isinstance(data, np.ndarray):
            self.data = data.astype(np.float32)
        else:
            self.data = np.array(data, dtype=np.float32)
        
        self.grad = None
        self.requires_grad = requires_grad
        self._children = []
        self._backward_fn = None
        self._op = ""
    
    @property
    def shape(self):
        return tuple(self.data.shape)
    
    def _ensure_grad(self):
        if self.grad is None:
            self.grad = Tensor(np.zeros_like(self.data), requires_grad=False)
    
    def __add__(self, other):
        if not isinstance(other, Tensor):
            other = Tensor(other)
        result_data = self.data + other.data
        out = Tensor(result_data, requires_grad=(self.requires_grad or other.requires_grad))
        return out
    
    def __mul__(self, other):
        if not isinstance(other, Tensor):
            other = Tensor(other)
        result_data = self.data * other.data
        out = Tensor(result_data, requires_grad=(self.requires_grad or other.requires_grad))
        return out
    
    def sum(self, axis=None, keepdim=False):
        result_data = np.sum(self.data, axis=axis, keepdims=keepdim)
        out = Tensor(result_data, requires_grad=self.requires_grad)
        return out
    
    def backward(self):
        # Simplified backward pass
        if self.grad is None:
            self.grad = Tensor(np.ones_like(self.data), requires_grad=False)
    
    def zero_grad(self):
        if self.grad is not None:
            self.grad.data.fill(0)

# ============================================================================
# PART 1: THE MATHEMATICS OF CONVOLUTION - SPATIAL INTELLIGENCE
# ============================================================================

print("ğŸ“š PART 1: THE MATHEMATICS OF CONVOLUTION")
print("="*60)

print("""
ğŸ¯ WHY CONVOLUTION CHANGED EVERYTHING

Before CNNs (1980s):
â€¢ Dense layers: Every pixel connected to every neuron
â€¢ MNIST (28Ã—28): 784 â†’ 128 = 100,352 parameters per layer!
â€¢ No spatial understanding: pixel (0,0) treated same as pixel (13,14)
â€¢ Massive overfitting on image tasks

After CNNs (2012+):
â€¢ Local connections: Small filters slide across image
â€¢ Parameter sharing: Same filter used everywhere
â€¢ Spatial hierarchy: Low-level â†’ high-level features
â€¢ Translation invariance: Object recognition regardless of position

ğŸ§® THE CONVOLUTION OPERATION

Mathematical Definition:
    (f * g)[i,j] = Î£_m Î£_n f[i+m, j+n] Ã— g[m,n]

Where:
â€¢ f = input image/feature map
â€¢ g = kernel/filter (learnable weights)
â€¢ * = convolution operator (not multiplication!)

ğŸ” CONVOLUTION VS CORRELATION:

Convolution (flips kernel):
    output[i,j] = Î£ input[i-m, j-n] Ã— kernel[m,n]

Cross-correlation (no flip):
    output[i,j] = Î£ input[i+m, j+n] Ã— kernel[m,n]

Deep learning uses cross-correlation but calls it "convolution"!
(The flip doesn't matter since kernels are learned)

ğŸ¨ WHAT DO CONVOLUTIONAL FILTERS LEARN?

Layer 1 filters (low-level):
â€¢ Edge detectors: vertical, horizontal, diagonal lines
â€¢ Color blob detectors
â€¢ Simple texture patterns

Layer 2 filters (mid-level):
â€¢ Corner detectors
â€¢ Simple shapes (circles, rectangles)
â€¢ Texture combinations

Layer 3+ filters (high-level):
â€¢ Object parts (eyes, wheels, fur patterns)
â€¢ Complex textures
â€¢ Semantic concepts

ğŸ§  KEY CONVOLUTION PROPERTIES:

1. TRANSLATION EQUIVARIANCE:
   If input shifts, output shifts by same amount
   conv(shift(x)) = shift(conv(x))

2. PARAMETER SHARING:
   Same filter applied everywhere
   Dramatically reduces parameters: 784Ã—128 â†’ 3Ã—3Ã—channels

3. LOCAL CONNECTIVITY:
   Each output depends on small local region
   Respects spatial structure of images

4. HIERARCHICAL FEATURES:
   Deep networks build complex features from simple ones
   
ğŸ”¬ CONVOLUTION DIMENSIONS:

Input: [N, C_in, H, W]
â€¢ N = batch size
â€¢ C_in = input channels (3 for RGB, 1 for grayscale)
â€¢ H, W = height, width

Kernel: [C_out, C_in, K_H, K_W]
â€¢ C_out = number of output channels
â€¢ C_in = input channels (must match input)
â€¢ K_H, K_W = kernel height, width

Output: [N, C_out, H_out, W_out]
â€¢ H_out = (H + 2Ã—pad - K_H) / stride + 1
â€¢ W_out = (W + 2Ã—pad - K_W) / stride + 1

ğŸ¯ HYPERPARAMETERS EXPLAINED:

KERNEL SIZE:
â€¢ 1Ã—1: Channel mixing, no spatial processing
â€¢ 3Ã—3: Most common, good balance of receptive field and efficiency
â€¢ 5Ã—5, 7Ã—7: Larger receptive field, more parameters
â€¢ 11Ã—11: Very large, used in early layers of some architectures

STRIDE:
â€¢ 1: No downsampling, output same spatial size
â€¢ 2: Halve spatial dimensions (common)
â€¢ >2: Aggressive downsampling

PADDING:
â€¢ 'valid': No padding, output smaller than input
â€¢ 'same': Pad to keep output same size as input/stride
â€¢ Custom: Specify exact padding amounts

DILATION:
â€¢ 1: Standard convolution
â€¢ >1: Dilated/atrous convolution, increases receptive field without more parameters

ğŸŒŸ THE RECEPTIVE FIELD:

The receptive field is the region of input that affects a single output pixel.

For a stack of 3Ã—3 convolutions:
â€¢ Layer 1: 3Ã—3 receptive field
â€¢ Layer 2: 5Ã—5 receptive field  
â€¢ Layer 3: 7Ã—7 receptive field
â€¢ Layer n: (2n+1)Ã—(2n+1) receptive field

This is how CNNs see increasingly large parts of the image!

ğŸ”¥ WHY 3Ã—3 KERNELS DOMINATE:

â€¢ Two 3Ã—3 convs = one 5Ã—5 conv in receptive field
â€¢ But 2Ã—(3Ã—3) = 18 parameters vs 5Ã—5 = 25 parameters
â€¢ More non-linearity (2 ReLUs vs 1 ReLU)
â€¢ Computational efficiency on modern hardware

This insight from VGGNet changed CNN design forever!
""")

# ============================================================================
# PART 2: IMPLEMENTING 2D CONVOLUTION - THE CORE OPERATION
# ============================================================================

print("\n\nğŸ’» PART 2: IMPLEMENTING 2D CONVOLUTION")
print("="*60)

print("""
ğŸ—ï¸ CONVOLUTION IMPLEMENTATION CHALLENGES

Building conv2d from scratch requires handling:

1. ğŸ¯ FORWARD PASS: Efficient convolution computation
2. ğŸ”„ BACKWARD PASS: Gradients w.r.t. input and kernel
3. ğŸ“ PADDING: Border handling for different padding modes
4. ğŸƒ STRIDE: Subsampling during convolution
5. ğŸ“Š BATCHING: Multiple samples simultaneously
6. ğŸ­ MULTIPLE CHANNELS: RGB images, feature maps

ğŸ§® NAIVE CONVOLUTION (EDUCATIONAL):

def naive_conv2d(input, kernel):
    N, C_in, H, W = input.shape
    C_out, C_in, K_H, K_W = kernel.shape
    H_out = H - K_H + 1
    W_out = W - K_W + 1
    
    output = np.zeros((N, C_out, H_out, W_out))
    
    for n in range(N):                    # Batch
        for c_out in range(C_out):        # Output channels
            for c_in in range(C_in):      # Input channels
                for h in range(H_out):    # Output height
                    for w in range(W_out): # Output width
                        for kh in range(K_H): # Kernel height
                            for kw in range(K_W): # Kernel width
                                output[n,c_out,h,w] += input[n,c_in,h+kh,w+kw] * kernel[c_out,c_in,kh,kw]
    
    return output

This is 7 nested loops! Too slow for real use, but shows the math clearly.

âš¡ EFFICIENT CONVOLUTION:

Real implementations use:
â€¢ im2col: Convert convolution to matrix multiplication
â€¢ FFT: Fast Fourier Transform for large kernels
â€¢ Winograd: Algorithmic optimization for 3Ã—3 kernels
â€¢ Hardware: Specialized convolution units on GPUs

We'll implement im2col approach for clarity and reasonable efficiency.

ğŸ¯ IM2COL ALGORITHM:

1. Convert input patches to columns
2. Flatten kernel to row vector
3. Matrix multiply: result = kernel @ im2col(input)
4. Reshape result to output feature map

This transforms convolution into highly optimized GEMM (matrix multiply)!

ğŸ”¬ GRADIENT COMPUTATION:

Forward: Y = conv(X, W)

Gradients:
â€¢ âˆ‚L/âˆ‚X = conv(âˆ‚L/âˆ‚Y, rot180(W)) [convolution with rotated kernel]
â€¢ âˆ‚L/âˆ‚W = conv(X, âˆ‚L/âˆ‚Y) [convolution of input with output gradients]

The key insight: convolution gradients are also convolutions!
""")

def im2col(input_data, kernel_h, kernel_w, stride=1, padding=0):
    """
    Convert input data to column matrix for efficient convolution.
    
    This is the key trick that makes convolution fast by converting it
    to matrix multiplication.
    
    Args:
        input_data: Input tensor [N, C, H, W]
        kernel_h, kernel_w: Kernel dimensions
        stride: Convolution stride
        padding: Padding amount
        
    Returns:
        Column matrix where each column is a flattened patch
    """
    N, C, H, W = input_data.shape
    
    # Calculate output dimensions
    out_h = (H + 2 * padding - kernel_h) // stride + 1
    out_w = (W + 2 * padding - kernel_w) // stride + 1
    
    # Add padding if needed
    if padding > 0:
        padded = np.pad(input_data, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')
    else:
        padded = input_data
    
    # Create column matrix
    col = np.zeros((N, C, kernel_h, kernel_w, out_h, out_w), dtype=input_data.dtype)
    
    for j in range(kernel_h):
        j_max = j + stride * out_h
        for i in range(kernel_w):
            i_max = i + stride * out_w
            col[:, :, j, i, :, :] = padded[:, :, j:j_max:stride, i:i_max:stride]
    
    # Reshape to 2D matrix: (N*C*K_H*K_W, out_h*out_w)
    col = col.transpose((0, 4, 5, 1, 2, 3)).reshape((N * out_h * out_w, -1))
    
    return col

def col2im(col, input_shape, kernel_h, kernel_w, stride=1, padding=0):
    """
    Inverse of im2col - convert column matrix back to input format.
    
    Used in backward pass to convert gradients back to input shape.
    """
    N, C, H, W = input_shape
    out_h = (H + 2 * padding - kernel_h) // stride + 1
    out_w = (W + 2 * padding - kernel_w) // stride + 1
    
    # Reshape column matrix
    col = col.reshape(N, out_h, out_w, C, kernel_h, kernel_w).transpose((0, 3, 4, 5, 1, 2))
    
    # Initialize padded output
    padded_h = H + 2 * padding
    padded_w = W + 2 * padding
    img = np.zeros((N, C, padded_h, padded_w), dtype=col.dtype)
    
    # Add values back to image
    for j in range(kernel_h):
        j_max = j + stride * out_h
        for i in range(kernel_w):
            i_max = i + stride * out_w
            img[:, :, j:j_max:stride, i:i_max:stride] += col[:, :, j, i, :, :]
    
    # Remove padding
    if padding > 0:
        return img[:, :, padding:-padding, padding:-padding]
    else:
        return img

class Conv2d:
    """
    2D Convolutional layer.
    
    This is the fundamental building block of convolutional neural networks.
    It applies learnable filters to detect spatial patterns in input data.
    
    Mathematical Operation:
        For each output position (i,j) and output channel c_out:
        output[c_out,i,j] = Î£_c_in Î£_kh Î£_kw input[c_in,i+kh,j+kw] * weight[c_out,c_in,kh,kw] + bias[c_out]
    
    Key Features:
        - Parameter sharing: Same filter applied across spatial dimensions
        - Local connectivity: Each output depends on local input region
        - Translation equivariance: Shifted input produces shifted output
        
    Args:
        in_channels: Number of input channels
        out_channels: Number of output channels (number of filters)
        kernel_size: Size of convolution kernel
        stride: Stride of convolution (default: 1)
        padding: Padding added to input (default: 0)
        bias: Whether to include bias term (default: True)
        
    Shape:
        Input: [N, in_channels, H, W]
        Output: [N, out_channels, H_out, W_out]
        
        Where:
        H_out = (H + 2*padding - kernel_size) // stride + 1
        W_out = (W + 2*padding - kernel_size) // stride + 1
        
    Example:
        >>> conv = Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        >>> x = Tensor(np.random.randn(1, 3, 224, 224))  # Batch of RGB images
        >>> y = conv(x)  # Shape: [1, 32, 224, 224]
    """
    
    def __init__(self, 
                 in_channels: int,
                 out_channels: int, 
                 kernel_size: Union[int, Tuple[int, int]],
                 stride: Union[int, Tuple[int, int]] = 1,
                 padding: Union[int, Tuple[int, int]] = 0,
                 bias: bool = True):
        """
        Initialize Conv2d layer.
        
        Args:
            in_channels: Number of input channels
            out_channels: Number of output filters
            kernel_size: Convolution kernel size
            stride: Convolution stride
            padding: Input padding
            bias: Whether to use bias
        """
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # Handle kernel_size as int or tuple
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size
            
        # Handle stride as int or tuple
        if isinstance(stride, int):
            self.stride = (stride, stride)
        else:
            self.stride = stride
            
        # Handle padding as int or tuple
        if isinstance(padding, int):
            self.padding = (padding, padding)
        else:
            self.padding = padding
            
        self.use_bias = bias
        
        # Initialize weights using He initialization (good for ReLU)
        kernel_h, kernel_w = self.kernel_size
        fan_in = in_channels * kernel_h * kernel_w
        std = math.sqrt(2.0 / fan_in)  # He initialization
        
        weight_shape = (out_channels, in_channels, kernel_h, kernel_w)
        self.weight = Tensor(np.random.normal(0, std, weight_shape), requires_grad=True)
        
        if bias:
            self.bias = Tensor(np.zeros(out_channels), requires_grad=True)
        else:
            self.bias = None
    
    def __call__(self, x: Tensor) -> Tensor:
        """
        Forward pass through convolution layer.
        
        Args:
            x: Input tensor [N, C_in, H, W]
            
        Returns:
            Output tensor [N, C_out, H_out, W_out]
        """
        return self.forward(x)
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Perform 2D convolution forward pass.
        
        This uses the im2col approach for efficiency:
        1. Convert input patches to column matrix
        2. Reshape weight to 2D matrix
        3. Perform matrix multiplication
        4. Reshape result to proper output format
        """
        N, C_in, H, W = x.shape
        C_out, C_in_w, K_H, K_W = self.weight.shape
        
        # Validate input channels
        if C_in != C_in_w:
            raise ValueError(f"Input channels {C_in} doesn't match weight channels {C_in_w}")
        
        # Calculate output dimensions
        stride_h, stride_w = self.stride
        pad_h, pad_w = self.padding
        
        H_out = (H + 2 * pad_h - K_H) // stride_h + 1
        W_out = (W + 2 * pad_w - K_W) // stride_w + 1
        
        # Convert input to column matrix
        col = im2col(x.data, K_H, K_W, stride_h, pad_h)
        
        # Reshape weight to 2D matrix
        weight_col = self.weight.data.reshape(C_out, -1)
        
        # Perform convolution as matrix multiplication
        out = weight_col @ col.T  # Shape: (C_out, N*H_out*W_out)
        
        # Add bias if present
        if self.use_bias:
            out = out + self.bias.data.reshape(-1, 1)
        
        # Reshape to proper output format
        out = out.reshape(C_out, N, H_out, W_out).transpose(1, 0, 2, 3)
        
        # Create output tensor with gradient tracking
        output = Tensor(out, requires_grad=(x.requires_grad or self.weight.requires_grad))
        output._children = [x, self.weight]
        if self.use_bias:
            output._children.append(self.bias)
        output._op = 'conv2d'
        
        # Store information needed for backward pass
        self._last_input = x
        self._last_col = col
        self._last_output_shape = (N, C_out, H_out, W_out)
        
        def _backward():
            """Compute gradients for convolution."""
            if output.grad is None:
                return
                
            grad_out = output.grad.data  # Shape: (N, C_out, H_out, W_out)
            
            # Gradient w.r.t. bias
            if self.use_bias and self.bias.requires_grad:
                self.bias._ensure_grad()
                # Sum over batch, height, width dimensions
                grad_bias = np.sum(grad_out, axis=(0, 2, 3))
                self.bias.grad.data += grad_bias
            
            # Gradient w.r.t. weight
            if self.weight.requires_grad:
                self.weight._ensure_grad()
                
                # Reshape grad_out for matrix multiplication
                grad_out_col = grad_out.transpose(1, 0, 2, 3).reshape(C_out, -1)
                
                # grad_weight = grad_out_col @ col
                grad_weight = grad_out_col @ self._last_col
                grad_weight = grad_weight.reshape(self.weight.shape)
                
                self.weight.grad.data += grad_weight
            
            # Gradient w.r.t. input
            if x.requires_grad:
                x._ensure_grad()
                
                # grad_input = weight^T @ grad_out
                weight_col = self.weight.data.reshape(C_out, -1)
                grad_out_col = grad_out.transpose(1, 0, 2, 3).reshape(C_out, -1)
                
                grad_col = weight_col.T @ grad_out_col  # Shape: (C_in*K_H*K_W, N*H_out*W_out)
                grad_col = grad_col.T  # Shape: (N*H_out*W_out, C_in*K_H*K_W)
                
                # Convert back to input shape using col2im
                grad_input = col2im(grad_col, x.shape, K_H, K_W, stride_h, pad_h)
                
                x.grad.data += grad_input
        
        output._backward_fn = _backward
        return output
    
    def parameters(self) -> List[Tensor]:
        """Return list of learnable parameters."""
        params = [self.weight]
        if self.use_bias:
            params.append(self.bias)
        return params
    
    def __repr__(self) -> str:
        return (f"Conv2d({self.in_channels}, {self.out_channels}, "
                f"kernel_size={self.kernel_size}, stride={self.stride}, "
                f"padding={self.padding}, bias={self.use_bias})")

print("\nâœ… 2D CONVOLUTION IMPLEMENTED!")

# ============================================================================
# PART 3: POOLING OPERATIONS - SPATIAL DOWNSAMPLING
# ============================================================================

print("\n\nğŸ“š PART 3: POOLING OPERATIONS")
print("="*60)

print("""
ğŸ¯ WHY POOLING IS ESSENTIAL

Pooling operations provide:

1. ğŸ“‰ SPATIAL DOWNSAMPLING: Reduce feature map size
2. ğŸ­ TRANSLATION INVARIANCE: Small shifts don't affect output
3. ğŸ’¾ MEMORY EFFICIENCY: Fewer parameters in subsequent layers
4. ğŸ” LARGER RECEPTIVE FIELDS: See bigger parts of input
5. ğŸ›¡ï¸ NOISE REDUCTION: Average out small variations

ğŸ§® TYPES OF POOLING:

1. MAX POOLING:
   output[i,j] = max(input[i*s:i*s+k, j*s:j*s+k])
   
   â€¢ Preserves strongest activations
   â€¢ Translation invariant for small shifts
   â€¢ Most common in classification CNNs

2. AVERAGE POOLING:
   output[i,j] = mean(input[i*s:i*s+k, j*s:j*s+k])
   
   â€¢ Smooths activations
   â€¢ Better for regression tasks
   â€¢ Often used in final layers (Global Average Pooling)

3. ADAPTIVE POOLING:
   â€¢ Output size fixed regardless of input size
   â€¢ Useful for handling variable input sizes

ğŸ”¬ POOLING HYPERPARAMETERS:

Kernel Size:
â€¢ 2Ã—2: Most common, halves spatial dimensions
â€¢ 3Ã—3: Larger receptive field, more aggressive downsampling
â€¢ Global: Pool entire feature map to single value

Stride:
â€¢ Same as kernel size: Non-overlapping (standard)
â€¢ Smaller than kernel size: Overlapping pooling
â€¢ 1: No downsampling (rare)

Padding:
â€¢ Usually 0 (no padding)
â€¢ Sometimes used to maintain exact output sizes

ğŸ¯ MAX POOLING GRADIENT:

Forward: y[i,j] = max(x[region])
Backward: Only max element gets gradient, others get 0

This creates sparse gradients that flow only through "winning" paths.

ğŸŒŸ GLOBAL AVERAGE POOLING:

Instead of flattening before final classifier:
   Traditional: Conv â†’ Flatten â†’ Dense â†’ Output
   GAP: Conv â†’ GlobalAvgPool â†’ Dense â†’ Output

Benefits:
â€¢ Fewer parameters (no huge dense layer)
â€¢ Better generalization
â€¢ Spatial information preserved until end

ğŸ”¥ MODERN ALTERNATIVES:

â€¢ Strided Convolutions: Replace pooling with stride>1 convs
â€¢ Dilated Convolutions: Increase receptive field without downsampling
â€¢ Attention Pooling: Learn where to focus attention
â€¢ Learnable Pooling: