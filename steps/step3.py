"""
LESSON 3: NEURAL NETWORK LAYERS
================================
From Tensors to Deep Learning Building Blocks

üéØ LEARNING OBJECTIVES:
1. Understand what neural network layers are and why we need them
2. Learn proper weight initialization strategies (Xavier, He, etc.)
3. Implement Linear/Dense layers with automatic gradient computation
4. Master advanced activation functions and their mathematical properties
5. Build loss functions that drive learning
6. Create a modular layer system like PyTorch/TensorFlow
7. Understand computational graphs and automatic differentiation

üìö PRE-REQUISITES:
- Completed Lesson 1 (Value class)
- Completed Lesson 2 (Tensor class with broadcasting)
- Understanding of matrix multiplication and gradients
- Basic knowledge of neural network concepts

üé® END GOAL PREVIEW:
By the end of this lesson, you'll have the core components to build any neural network:
- Linear layers (the foundation of all NNs)
- Advanced activations (GELU, Swish, etc.)
- Loss functions (CrossEntropy, MSE)
- A modular system for composing complex architectures
- Proper weight initialization for stable training
"""

import numpy as np
import math
from typing import Union, Tuple, List, Optional, Any, Callable
import warnings
warnings.filterwarnings('ignore')

# Import our Tensor class from Lesson 2
# (In practice, this would be: from lesson2 import Tensor)
# For this lesson, we'll include a simplified version

class Tensor:
    """Simplified Tensor class from Lesson 2 for this lesson."""
    
    def __init__(self, data, requires_grad=False):
        if isinstance(data, Tensor):
            self.data = data.data.copy()
        elif isinstance(data, np.ndarray):
            self.data = data.astype(np.float32)
        else:
            self.data = np.array(data, dtype=np.float32)
        
        self.grad = None
        self.requires_grad = requires_grad
        self._children = []
        self._backward_fn = None
        self._op = ""
    
    def __repr__(self):
        grad_info = f", grad_fn={self._op}" if self._op else ""
        return f"Tensor({self.data}{grad_info}, requires_grad={self.requires_grad})"
    
    @property
    def shape(self):
        return tuple(self.data.shape)
    
    def _ensure_grad(self):
        if self.grad is None:
            self.grad = Tensor(np.zeros_like(self.data), requires_grad=False)
    
    def _handle_broadcasting_backward(self, grad):
        if self.data.ndim == 0:
            return np.sum(grad)
        ndims_added = grad.ndim - self.data.ndim
        for _ in range(ndims_added):
            grad = grad.sum(axis=0)
        for i, (grad_dim, self_dim) in enumerate(zip(grad.shape, self.data.shape)):
            if self_dim == 1 and grad_dim > 1:
                grad = grad.sum(axis=i, keepdims=True)
        return grad
    
    def __add__(self, other):
        if not isinstance(other, Tensor):
            other = Tensor(other)
        result_data = self.data + other.data
        out = Tensor(result_data, requires_grad=(self.requires_grad or other.requires_grad))
        out._children = [self, other]
        out._op = 'add'
        
        def _backward():
            if self.requires_grad:
                self._ensure_grad()
                grad = self._handle_broadcasting_backward(out.grad.data)
                self.grad.data += grad
            if other.requires_grad:
                other._ensure_grad()
                grad = other._handle_broadcasting_backward(out.grad.data)
                other.grad.data += grad
        
        out._backward_fn = _backward
        return out
    
    def __mul__(self, other):
        if not isinstance(other, Tensor):
            other = Tensor(other)
        result_data = self.data * other.data
        out = Tensor(result_data, requires_grad=(self.requires_grad or other.requires_grad))
        out._children = [self, other]
        out._op = 'mul'
        
        def _backward():
            if self.requires_grad:
                self._ensure_grad()
                grad = out.grad.data * other.data
                grad = self._handle_broadcasting_backward(grad)
                self.grad.data += grad
            if other.requires_grad:
                other._ensure_grad()
                grad = out.grad.data * self.data
                grad = other._handle_broadcasting_backward(grad)
                other.grad.data += grad
        
        out._backward_fn = _backward
        return out
    
    def __matmul__(self, other):
        result_data = self.data @ other.data
        out = Tensor(result_data, requires_grad=(self.requires_grad or other.requires_grad))
        out._children = [self, other]
        out._op = 'matmul'
        
        def _backward():
            if self.requires_grad:
                self._ensure_grad()
                grad = out.grad.data @ other.data.T
                self.grad.data += grad
            if other.requires_grad:
                other._ensure_grad()
                grad = self.data.T @ out.grad.data
                other.grad.data += grad
        
        out._backward_fn = _backward
        return out
    
    def sum(self, axis=None, keepdim=False):
        result_data = np.sum(self.data, axis=axis, keepdims=keepdim)
        out = Tensor(result_data, requires_grad=self.requires_grad)
        out._children = [self]
        out._op = 'sum'
        
        def _backward():
            if self.requires_grad:
                self._ensure_grad()
                grad = out.grad.data
                if not keepdim and axis is not None:
                    if isinstance(axis, int):
                        axes = [axis]
                    else:
                        axes = list(axis)
                    for ax in sorted(axes):
                        if ax < 0:
                            ax = len(self.shape) + ax
                        grad = np.expand_dims(grad, ax)
                grad = np.broadcast_to(grad, self.data.shape)
                self.grad.data += grad
        
        out._backward_fn = _backward
        return out
    
    def mean(self, axis=None, keepdim=False):
        if axis is None:
            count = self.data.size
        else:
            if isinstance(axis, int):
                count = self.shape[axis]
            else:
                count = np.prod([self.shape[ax] for ax in axis])
        return self.sum(axis=axis, keepdim=keepdim) / count
    
    def relu(self):
        result_data = np.maximum(0, self.data)
        out = Tensor(result_data, requires_grad=self.requires_grad)
        out._children = [self]
        out._op = 'relu'
        
        def _backward():
            if self.requires_grad:
                self._ensure_grad()
                grad = (self.data > 0).astype(np.float32) * out.grad.data
                self.grad.data += grad
        
        out._backward_fn = _backward
        return out
    
    def backward(self):
        topo = []
        visited = set()
        
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        
        build_topo(self)
        
        if self.grad is None:
            self.grad = Tensor(np.ones_like(self.data), requires_grad=False)
        
        for v in reversed(topo):
            if v._backward_fn:
                v._backward_fn()
    
    def zero_grad(self):
        if self.grad is not None:
            self.grad.data.fill(0)
    
    # Additional methods we'll need
    def __neg__(self): return self * -1
    def __sub__(self, other): return self + (-other)
    def __truediv__(self, other): return self * (other ** -1)
    def __pow__(self, exponent): 
        return Tensor(self.data ** exponent, requires_grad=self.requires_grad)
    def __radd__(self, other): return self.__add__(other)
    def __rmul__(self, other): return self.__mul__(other)

# ============================================================================
# PART 1: UNDERSTANDING NEURAL NETWORK LAYERS - THE THEORY
# ============================================================================

print("üìö PART 1: UNDERSTANDING NEURAL NETWORK LAYERS")
print("="*60)

print("""
ü§î WHY WE NEED LAYERS

Your Tensor class can do math operations, but neural networks are more than just math.
They're composed of LAYERS - reusable building blocks that transform data.

Think of layers like LEGO blocks:
‚Ä¢ Each block has a specific function
‚Ä¢ You can combine them in countless ways
‚Ä¢ The same block can be reused in different positions
‚Ä¢ Complex structures emerge from simple components

üßÆ WHAT IS A NEURAL NETWORK LAYER?

A layer is a function that:
1. Takes input tensors
2. Applies a mathematical transformation
3. Produces output tensors
4. Has learnable parameters (weights, biases)
5. Can compute gradients for all parameters

Mathematical notation:
   output = layer(input; parameters)
   
   Where:
   - input: data flowing through the network
   - parameters: learnable weights and biases
   - output: transformed data

üèóÔ∏è TYPES OF LAYERS (The Building Blocks):

1. LINEAR/DENSE LAYER (Most Important):
   output = input @ weights + bias
   
   Purpose: Learn linear transformations
   Use cases: Classification, regression, feature mixing
   
2. ACTIVATION LAYERS:
   output = activation_function(input)
   
   Purpose: Add non-linearity (without this, networks are just linear!)
   Examples: ReLU, Sigmoid, Tanh, GELU
   
3. NORMALIZATION LAYERS:
   output = normalize(input)
   
   Purpose: Stabilize training, prevent gradient problems
   Examples: BatchNorm, LayerNorm
   
4. CONVOLUTION LAYERS:
   output = conv2d(input, kernel)
   
   Purpose: Process spatial data (images, sequences)
   Use cases: Computer vision, pattern recognition
   
5. POOLING LAYERS:
   output = pool(input)
   
   Purpose: Reduce spatial dimensions, extract important features
   Examples: MaxPool, AvgPool
   
6. DROPOUT LAYERS:
   output = randomly_zero_some_inputs(input)
   
   Purpose: Prevent overfitting, improve generalization

üéØ WHY THE LINEAR LAYER IS KING

Every complex layer can be built from linear layers + activations:
‚Ä¢ Transformers: Multi-head attention = many linear layers
‚Ä¢ CNNs: Convolution = linear operation with weight sharing
‚Ä¢ RNNs: Recurrent connection = linear layer with memory

Master the linear layer, and you can build anything!

üî• THE MAGIC OF COMPOSITION

Simple layers ‚Üí Complex behaviors:

Single Layer:
   input ‚Üí Linear ‚Üí ReLU ‚Üí output
   (Can only learn simple patterns)

Multi-Layer (Deep Network):
   input ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí output
   (Can learn incredibly complex patterns!)

This is why they're called "deep" learning - the depth creates the intelligence!

üß† COMPUTATIONAL GRAPH PERSPECTIVE

Each layer creates nodes in a computational graph:

   x ‚Üí [Linear] ‚Üí h1 ‚Üí [ReLU] ‚Üí h2 ‚Üí [Linear] ‚Üí y
       ‚Üë          ‚Üë        ‚Üë        ‚Üë
     W1,b1     relu()    W2,b2    output

Gradients flow backward through this graph:
   ‚àÇL/‚àÇx ‚Üê [Linear] ‚Üê ‚àÇL/‚àÇh1 ‚Üê [ReLU] ‚Üê ‚àÇL/‚àÇh2 ‚Üê [Linear] ‚Üê ‚àÇL/‚àÇy
""")

# ============================================================================
# PART 2: WEIGHT INITIALIZATION - THE FOUNDATION OF TRAINING
# ============================================================================

print("\n\nüìö PART 2: WEIGHT INITIALIZATION STRATEGIES")
print("="*60)

print("""
üéØ THE CRITICAL IMPORTANCE OF INITIALIZATION

How you initialize weights determines whether your network will:
‚úÖ Train successfully and converge quickly
‚ùå Fail to learn anything (vanishing/exploding gradients)

Bad initialization = broken network, no matter how good your architecture!

üî¢ THE MATHEMATICAL PROBLEM

Consider a deep network with L layers:
   h‚ÇÅ = W‚ÇÅx + b‚ÇÅ
   h‚ÇÇ = W‚ÇÇh‚ÇÅ + b‚ÇÇ
   ...
   h‚Çó = W‚Çóh‚Çó‚Çã‚ÇÅ + b‚Çó

If weights are too small:
   ‚Ä¢ Activations shrink: |h·µ¢| ‚Üí 0 as i increases
   ‚Ä¢ Gradients vanish: ‚àÇL/‚àÇW‚ÇÅ ‚âà 0
   ‚Ä¢ Early layers don't learn

If weights are too large:
   ‚Ä¢ Activations explode: |h·µ¢| ‚Üí ‚àû as i increases  
   ‚Ä¢ Gradients explode: ‚àÇL/‚àÇW‚ÇÅ ‚Üí ‚àû
   ‚Ä¢ Training becomes unstable

üéØ INITIALIZATION STRATEGIES

1. ZERO INITIALIZATION (‚ùå Never use this):
   W = 0
   
   Problem: All neurons compute the same thing!
   Result: Network learns nothing
   
2. RANDOM INITIALIZATION (‚ùå Usually bad):
   W ~ Normal(0, 1)
   
   Problem: Variance doesn't account for layer size
   Result: Vanishing or exploding gradients
   
3. XAVIER/GLOROT INITIALIZATION (‚úÖ Good for tanh/sigmoid):
   W ~ Normal(0, ‚àö(2/(fan_in + fan_out)))
   
   Goal: Keep variance constant across layers
   Best for: tanh, sigmoid activations
   
4. HE INITIALIZATION (‚úÖ Best for ReLU):
   W ~ Normal(0, ‚àö(2/fan_in))
   
   Goal: Account for ReLU killing half the neurons
   Best for: ReLU, LeakyReLU activations
   
5. LECUN INITIALIZATION (‚úÖ Good for SELU):
   W ~ Normal(0, ‚àö(1/fan_in))
   
   Goal: Maintain unit variance with SELU properties
   Best for: SELU activation

Where:
- fan_in = number of input connections to a neuron
- fan_out = number of output connections from a neuron

üßÆ THE MATH BEHIND HE INITIALIZATION

For ReLU activation: f(x) = max(0, x)

Variance analysis:
1. Input variance: Var[x] = œÉ¬≤
2. After linear layer: Var[Wx] = n √ó Var[W] √ó Var[x]
3. After ReLU: Var[ReLU(Wx)] ‚âà ¬Ω √ó Var[Wx] (ReLU zeros half the values)

To maintain Var[output] = Var[input]:
   ¬Ω √ó n √ó Var[W] √ó œÉ¬≤ = œÉ¬≤
   Var[W] = 2/n
   std[W] = ‚àö(2/n)

This is why He initialization uses ‚àö(2/fan_in)!

üé® BIAS INITIALIZATION

Biases are usually initialized to zero:
   b = 0

Why? Biases don't suffer from the same variance issues as weights.
However, some special cases:
‚Ä¢ ReLU layers: Sometimes initialize small positive values
‚Ä¢ LSTM forget gates: Initialize to 1 to help long-term memory
‚Ä¢ Output layers: Initialize based on expected output range
""")

def get_initializer(name: str, fan_in: int, fan_out: int) -> Callable:
    """
    Factory function for different weight initialization strategies.
    
    Args:
        name: Initialization strategy name
        fan_in: Number of input units
        fan_out: Number of output units
        
    Returns:
        Function that generates initialized weights
        
    Mathematical Background:
        Each initialization strategy balances the variance of activations
        to prevent vanishing/exploding gradients in deep networks.
    """
    
    if name.lower() == 'zeros':
        def zeros_init(shape):
            return np.zeros(shape, dtype=np.float32)
        return zeros_init
    
    elif name.lower() == 'xavier' or name.lower() == 'glorot':
        # Xavier/Glorot: ‚àö(2/(fan_in + fan_out))
        std = math.sqrt(2.0 / (fan_in + fan_out))
        def xavier_init(shape):
            return np.random.normal(0, std, shape).astype(np.float32)
        return xavier_init
    
    elif name.lower() == 'he' or name.lower() == 'kaiming':
        # He: ‚àö(2/fan_in) - best for ReLU
        std = math.sqrt(2.0 / fan_in)
        def he_init(shape):
            return np.random.normal(0, std, shape).astype(np.float32)
        return he_init
    
    elif name.lower() == 'lecun':
        # LeCun: ‚àö(1/fan_in) - good for SELU
        std = math.sqrt(1.0 / fan_in)
        def lecun_init(shape):
            return np.random.normal(0, std, shape).astype(np.float32)
        return lecun_init
    
    elif name.lower() == 'uniform':
        # Uniform Xavier: ¬±‚àö(6/(fan_in + fan_out))
        limit = math.sqrt(6.0 / (fan_in + fan_out))
        def uniform_init(shape):
            return np.random.uniform(-limit, limit, shape).astype(np.float32)
        return uniform_init
    
    else:
        raise ValueError(f"Unknown initialization: {name}")

print("\nüß™ INITIALIZATION COMPARISON:")

# Demonstrate different initializations
layer_size = (128, 64)  # 128 inputs, 64 outputs
fan_in, fan_out = layer_size

print(f"Layer shape: {layer_size} (fan_in={fan_in}, fan_out={fan_out})")

for init_name in ['xavier', 'he', 'lecun', 'uniform']:
    initializer = get_initializer(init_name, fan_in, fan_out)
    weights = initializer(layer_size)
    
    print(f"{init_name:8}: std={weights.std():.6f}, range=[{weights.min():.3f}, {weights.max():.3f}]")

# ============================================================================
# PART 3: LINEAR LAYER IMPLEMENTATION - THE CORNERSTONE
# ============================================================================

print("\n\nüíª PART 3: IMPLEMENTING THE LINEAR LAYER")
print("="*60)

print("""
üéØ DESIGN GOALS FOR THE LINEAR LAYER

The Linear layer is the most important building block. It must:

1. MATHEMATICAL CORRECTNESS: Implement y = xW + b perfectly
2. AUTOMATIC DIFFERENTIATION: Gradients flow correctly
3. PROPER INITIALIZATION: Use best practices for weight setup
4. MEMORY EFFICIENCY: Minimize unnecessary allocations
5. BROADCASTING SUPPORT: Handle different batch sizes
6. PYTORCH COMPATIBILITY: Same interface as torch.nn.Linear

üèóÔ∏è LINEAR LAYER ARCHITECTURE

Input:  x with shape (batch_size, in_features)
Weight: W with shape (in_features, out_features)  
Bias:   b with shape (out_features,)
Output: y with shape (batch_size, out_features)

Forward pass:  y = x @ W + b
Backward pass: 
   ‚àÇL/‚àÇx = ‚àÇL/‚àÇy @ W.T
   ‚àÇL/‚àÇW = x.T @ ‚àÇL/‚àÇy  
   ‚àÇL/‚àÇb = sum(‚àÇL/‚àÇy, axis=0)

üßÆ WHY THIS MATH WORKS

Matrix multiplication perspective:
   [batch_size, in_features] @ [in_features, out_features] = [batch_size, out_features]

Each output neuron computes:
   y_j = Œ£(x_i * W_ij) + b_j for i in range(in_features)

This is a learned weighted combination of all input features!

üîÑ GRADIENT FLOW EXPLANATION

1. Forward: Data flows x ‚Üí y
2. Backward: Gradients flow ‚àÇL/‚àÇy ‚Üí ‚àÇL/‚àÇx

The gradients tell us:
- ‚àÇL/‚àÇW: How to update weights to reduce loss
- ‚àÇL/‚àÇb: How to update biases to reduce loss  
- ‚àÇL/‚àÇx: How the loss depends on inputs (for previous layers)
""")

class Linear:
    """
    Linear/Dense layer: y = x @ W + b
    
    This is the fundamental building block of neural networks.
    It performs an affine transformation of the input.
    
    Mathematical Background:
        A linear layer computes a weighted sum of inputs plus a bias term.
        Each output neuron is connected to every input neuron, making this
        a "fully connected" or "dense" layer.
        
        Forward:  y = x @ W + b
        Where:
            x: input tensor [batch_size, in_features]
            W: weight matrix [in_features, out_features] 
            b: bias vector [out_features]
            y: output tensor [batch_size, out_features]
            
        Gradients:
            ‚àÇL/‚àÇW = x^T @ ‚àÇL/‚àÇy    (outer product of input and output gradients)
            ‚àÇL/‚àÇb = sum(‚àÇL/‚àÇy)     (sum gradients across batch dimension)
            ‚àÇL/‚àÇx = ‚àÇL/‚àÇy @ W^T    (chain rule through weight matrix)
    
    Args:
        in_features: Number of input features
        out_features: Number of output features  
        bias: Whether to include bias term (default: True)
        weight_init: Weight initialization strategy (default: 'he')
        bias_init: Bias initialization strategy (default: 'zeros')
        
    Attributes:
        weight: Learnable weight tensor [in_features, out_features]
        bias: Learnable bias tensor [out_features] (if bias=True)
        
    Example:
        >>> layer = Linear(784, 128)  # MNIST input to hidden layer
        >>> x = Tensor(np.random.randn(32, 784))  # Batch of 32 images
        >>> y = layer(x)  # Output shape: [32, 128]
    """
    
    def __init__(self, 
                 in_features: int,
                 out_features: int, 
                 bias: bool = True,
                 weight_init: str = 'he',
                 bias_init: str = 'zeros'):
        """
        Initialize Linear layer with proper weight initialization.
        
        Args:
            in_features: Size of input features
            out_features: Size of output features
            bias: Whether to use bias term
            weight_init: Weight initialization strategy
            bias_init: Bias initialization strategy
        """
        self.in_features = in_features
        self.out_features = out_features
        self.use_bias = bias
        
        # Initialize weights using specified strategy
        weight_initializer = get_initializer(weight_init, in_features, out_features)
        weight_data = weight_initializer((in_features, out_features))
        self.weight = Tensor(weight_data, requires_grad=True)
        
        # Initialize bias (if used)
        if bias:
            bias_initializer = get_initializer(bias_init, in_features, out_features)
            bias_data = bias_initializer((out_features,))
            self.bias = Tensor(bias_data, requires_grad=True)
        else:
            self.bias = None
    
    def __call__(self, x: Tensor) -> Tensor:
        """
        Forward pass through linear layer.
        
        Args:
            x: Input tensor [batch_size, in_features]
            
        Returns:
            Output tensor [batch_size, out_features]
            
        Raises:
            ValueError: If input shape is incompatible
        """
        # Validate input shape
        if x.shape[-1] != self.in_features:
            raise ValueError(f"Input size {x.shape[-1]} doesn't match layer input size {self.in_features}")
        
        # Forward pass: y = x @ W + b
        output = x @ self.weight
        
        if self.use_bias:
            output = output + self.bias
            
        return output
    
    def parameters(self) -> List[Tensor]:
        """Return list of learnable parameters."""
        params = [self.weight]
        if self.use_bias:
            params.append(self.bias)
        return params
    
    def zero_grad(self) -> None:
        """Zero gradients of all parameters."""
        for param in self.parameters():
            param.zero_grad()
    
    def __repr__(self) -> str:
        return f"Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias})"

print("\n‚úÖ LINEAR LAYER IMPLEMENTED!")

# ============================================================================
# PART 4: ADVANCED ACTIVATION FUNCTIONS - ADDING NON-LINEARITY
# ============================================================================

print("\n\nüìö PART 4: ADVANCED ACTIVATION FUNCTIONS")
print("="*60)

print("""
üéØ WHY ACTIVATION FUNCTIONS ARE CRUCIAL

Without activation functions, neural networks are just linear algebra:
   y = W‚ÇÉ(W‚ÇÇ(W‚ÇÅx + b‚ÇÅ) + b‚ÇÇ) + b‚ÇÉ = (W‚ÇÉW‚ÇÇW‚ÇÅ)x + (combined bias terms)

This is equivalent to a single linear layer! üò±

Activation functions add NON-LINEARITY, enabling networks to learn complex patterns.

üßÆ ACTIVATION FUNCTION PROPERTIES

Good activation functions should have:
1. NON-LINEARITY: Enable complex function approximation
2. DIFFERENTIABILITY: Allow gradient-based optimization
3. MONOTONICITY: Preserve input ordering (usually preferred)
4. RANGE: Appropriate output range for the task
5. COMPUTATIONAL EFFICIENCY: Fast to compute and differentiate
6. GRADIENT PRESERVATION: Avoid vanishing gradient problems

üìä THE ACTIVATION FUNCTION ZOO

1. ReLU (Rectified Linear Unit) - The Classic:
   f(x) = max(0, x)
   f'(x) = 1 if x > 0, else 0
   
   Pros: Simple, fast, solves vanishing gradients
   Cons: "Dead ReLU" problem (neurons can die)
   
2. Sigmoid - The Original:
   f(x) = 1 / (1 + e^(-x))
   f'(x) = f(x) * (1 - f(x))
   
   Pros: Smooth, outputs in [0,1]
   Cons: Vanishing gradients, not zero-centered
   
3. Tanh - The Improved Sigmoid:
   f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
   f'(x) = 1 - f(x)¬≤
   
   Pros: Zero-centered, smooth
   Cons: Still suffers from vanishing gradients
   
4. GELU (Gaussian Error Linear Unit) - The Modern Choice:
   f(x) = x * Œ¶(x)  where Œ¶ is the standard normal CDF
   Approximation: f(x) ‚âà 0.5 * x * (1 + tanh(‚àö(2/œÄ) * (x + 0.044715 * x¬≥)))
   
   Pros: Smooth, non-monotonic, used in BERT/GPT
   Cons: More expensive to compute
   
5. Swish - The Learnable:
   f(x) = x * sigmoid(Œ≤x)  where Œ≤ is learnable
   
   Pros: Self-gated, smooth, often outperforms ReLU
   Cons: More parameters, more computation
   
6. Mish - The Smooth Alternative:
   f(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^x))
   
   Pros: Smooth, unbounded above, bounded below
   Cons: Computationally expensive

üéØ WHEN TO USE WHICH ACTIVATION

‚Ä¢ ReLU: Default choice, fast training, most layers
‚Ä¢ GELU: Transformer models, when you need smoothness  
‚Ä¢ Swish: When you want to experiment with smoother alternatives
‚Ä¢ Sigmoid: Output layer for binary classification
‚Ä¢ Tanh: Hidden layers when you need zero-centered outputs
‚Ä¢ Softmax: Output layer for multi-class classification

üî• THE DEAD RELU PROBLEM

Problem: If a ReLU neuron's input is always negative, its gradient is always 0
Result: The neuron never updates and becomes "dead"

Solutions:
1. Leaky ReLU: f(x) = max(Œ±x, x) where Œ± ‚âà 0.01
2. ELU: f(x) = x if x > 0, else Œ±(e^x - 1)
3. Better initialization
4. Lower learning rates
""")

class ActivationFunction:
    """Base class for activation functions."""
    
    def __call__(self, x: Tensor) -> Tensor:
        raise NotImplementedError
    
    def __repr__(self) -> str:
        return self.__class__.__name__ + "()"

class ReLU(ActivationFunction):
    """
    ReLU activation: f(x) = max(0, x)
    
    The most popular activation function in deep learning.
    Simple, fast, and effective at preventing vanishing gradients.
    
    Mathematical Properties:
        - f(x) = max(0, x)
        - f'(x) = 1 if x > 0, else 0
        - Range: [0, ‚àû)
        - Non-saturating for positive inputs
        
    Advantages:
        - Computationally efficient
        - Sparse activations (many zeros)
        - Helps with vanishing gradient problem
        - Biologically inspired
        
    Disadvantages:
        - "Dead ReLU" problem
        - Not zero-centered
        - Not differentiable at x=0
    """
    
    def __call__(self, x: Tensor) -> Tensor:
        return x.relu()

class GELU(ActivationFunction):
    """
    GELU activation: f(x) = x * Œ¶(x)
    
    Where Œ¶(x) is the standard normal cumulative distribution function.
    Used in BERT, GPT, and other transformer models.
    
    Mathematical Background:
        The idea is to weight inputs by their percentile in a normal distribution.
        Inputs in the lower tail get smaller weights, creating a smooth cutoff.
        
        Exact: f(x) = x * P(X ‚â§ x) where X ~ N(0,1)
        Approximation: f(x) ‚âà 0.5 * x * (1 + tanh(‚àö(2/œÄ) * (x + 0.044715 * x¬≥)))
        
    Properties:
        - Smooth and differentiable everywhere
        - Non-monotonic (slight dip for negative values)
        - Self-gating (input gates itself)
        - Stochastic interpretation (like dropout)
    """
    
    def __call__(self, x: Tensor) -> Tensor:
        # GELU approximation: 0.5 * x * (1 + tanh(sqrt(2/œÄ) * (x + 0.044715 * x¬≥)))
        sqrt_2_over_pi = math.sqrt(2.0 / math.pi)
        
        # Compute the tanh argument: sqrt(2/œÄ) * (x + 0.044715 * x¬≥)
        x_cubed = x * x * x
        tanh_arg = sqrt_2_over_pi * (x + 0.044715 * x_cubed)
        
        # tanh approximation using our tensor operations
        # tanh(z) ‚âà z / (1 + |z|) for a rough approximation
        # For better accuracy, we'll use: tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
        
        # Clip to prevent overflow
        tanh_arg_data = np.clip(tanh_arg.data, -10, 10)
        tanh_val = Tensor(np.tanh(tanh_arg_data))
        
        return 0.5 * x * (1 + tanh_val)

class Swish(ActivationFunction):
    """
    Swish activation: f(x) = x * sigmoid(Œ≤x)
    
    A self-gated activation function discovered by Google.
    Often outperforms ReLU on deeper models.
    
    Mathematical Properties:
        - f(x) = x * œÉ(Œ≤x) where œÉ is sigmoid
        - Self-gating: input gates itself
        - Smooth and non-monotonic
        - Œ≤ can be learned or fixed (usually Œ≤=1)
        
    Advantages:
        - Smooth everywhere (unlike ReLU)
        - Unbounded above, bounded below
        - Non-monotonic allows for more complex patterns
        
    Disadvantages:
        - More computationally expensive than ReLU
        - Additional parameter Œ≤ to tune
    """
    
    def __init__(self, beta: float = 1.0):
        self.beta = beta
    
    def __call__(self, x: Tensor) -> Tensor:
        # Swish: x * sigmoid(Œ≤ * x)
        beta_x = self.beta * x
        
        # Sigmoid implementation: 1 / (1 + exp(-x))
        # Clip for numerical stability
        beta_x_data = np.clip(beta_x.data, -500, 500)
        sigmoid_val = Tensor(1.0 / (1.0 + np.exp(-beta_x_data)))
        
        return x * sigmoid_val
    
    def __repr__(self) -> str:
        return f"Swish(beta={self.beta})"

class LeakyReLU(ActivationFunction):
    """
    Leaky ReLU activation: f(x) = max(Œ±x, x)
    
    A variant of ReLU that allows small negative values to pass through.
    Helps solve the "dead ReLU" problem.
    
    Mathematical Properties:
        - f(x) = x if x > 0, else Œ±*x
        - f'(x) = 1 if x > 0, else Œ±
        - Typical Œ± = 0.01 (1% leakage)
        
    Advantages:
        - Solves dead ReLU problem
        - Still very fast to compute
        - Allows gradient flow for negative inputs
        
    Disadvantages:
        - Additional hyperparameter Œ±
        - May not always outperform ReLU
    """
    
    def __init__(self, alpha: float = 0.01):
        self.alpha = alpha
    
    def __call__(self, x: Tensor) -> Tensor:
        # LeakyReLU: max(Œ±*x, x)
        alpha_x = self.alpha * x
        
        # Use element-wise maximum
        result_data = np.maximum(alpha_x.data, x.data)
        out = Tensor(result_data, requires_grad=x.requires_grad)
        out._children = [x]
        out._op = 'leaky_relu'
        
        def _backward():
            if x.requires_grad:
                x._ensure_grad()
                # Gradient: 1 if x > 0, else Œ±
                grad = np.where(x.data > 0, 1.0, self.alpha) * out.grad.data
                x.grad.data += grad
        
        out._backward_fn = _backward
        return out
    
    def __repr__(self) -> str:
        return f"LeakyReLU(alpha={self.alpha})"

class Tanh(ActivationFunction):
    """
    Tanh activation: f(x) = tanh(x)
    
    Hyperbolic tangent function, a scaled and shifted sigmoid.
    
    Mathematical Properties:
        - f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
        - f'(x) = 1 - tanh¬≤(x)
        - Range: (-1, 1)
        - Zero-centered output
        
    Advantages:
        - Zero-centered (unlike sigmoid)
        - Smooth and differentiable
        - Bounded output
        
    Disadvantages:
        - Still suffers from vanishing gradients
        - Slower than ReLU
    """
    
    def __call__(self, x: Tensor) -> Tensor:
        # Clip for numerical stability
        x_data = np.clip(x.data, -500, 500)
        result_data = np.tanh(x_data)
        
        out = Tensor(result_data, requires_grad=x.requires_grad)
        out._children = [x]
        out._op = 'tanh'
        
        def _backward():
            if x.requires_grad:
                x._ensure_grad()
                # Gradient: 1 - tanh¬≤(x)
                grad = (1 - result_data ** 2) * out.grad.data
                x.grad.data += grad
        
        out._backward_fn = _backward
        return out

class Sigmoid(ActivationFunction):
    """
    Sigmoid activation: f(x) = 1 / (1 + e^(-x))
    
    The classic activation function, especially useful for binary classification.
    
    Mathematical Properties:
        - f(x) = œÉ(x) = 1 / (1 + e^(-x))
        - f'(x) = œÉ(x) * (1 - œÉ(x))
        - Range: (0, 1)
        - S-shaped curve
        
    Advantages:
        - Smooth and differentiable
        - Output in (0,1) - good for probabilities
        - Well-understood mathematically
        
    Disadvantages:
        - Vanishing gradient problem
        - Not zero-centered
        - Computationally expensive
    """
    
    def __call__(self, x: Tensor) -> Tensor:
        # Sigmoid: 1 / (1 + exp(-x))
        x_data = np.clip(x.data, -500, 500)
        result_data = 1.0 / (1.0 + np.exp(-x_data))
        
        out = Tensor(result_data, requires_grad=x.requires_grad)
        out._children = [x]
        out._op = 'sigmoid'
        
        def _backward():
            if x.requires_grad:
                x._ensure_grad()
                # Gradient: œÉ(x) * (1 - œÉ(x))
                grad = result_data * (1 - result_data) * out.grad.data
                x.grad.data += grad
        
        out._backward_fn = _backward
        return out

print("\n‚úÖ ACTIVATION FUNCTIONS IMPLEMENTED!")
print("Available activations: ReLU, GELU, Swish, LeakyReLU, Tanh, Sigmoid")

# ============================================================================
# PART 5: LOSS FUNCTIONS - THE TRAINING OBJECTIVES
# ============================================================================

print("\n\nüìö PART 5: LOSS FUNCTIONS - WHAT DRIVES LEARNING")
print("="*60)

print("""
üéØ WHY LOSS FUNCTIONS ARE THE HEART OF LEARNING

Loss functions define WHAT the network should learn:
‚Ä¢ They measure how "wrong" the network's predictions are
‚Ä¢ They provide gradients that guide parameter updates
‚Ä¢ They determine what kind of task the network can solve

Without a loss function, there's no signal for learning!

üßÆ TYPES OF LEARNING PROBLEMS

1. REGRESSION: Predict continuous values
   Examples: House prices, temperature, stock values
   Common losses: Mean Squared Error (MSE), Mean Absolute Error
   
2. BINARY CLASSIFICATION: Yes/no decisions
   Examples: Email spam detection, medical diagnosis
   Common losses: Binary Cross-Entropy, Hinge Loss
   
3. MULTI-CLASS CLASSIFICATION: Pick one category
   Examples: Image recognition, sentiment analysis
   Common losses: Categorical Cross-Entropy, Sparse Cross-Entropy
   
4. MULTI-LABEL CLASSIFICATION: Multiple categories possible
   Examples: Image tagging, document classification
   Common losses: Binary Cross-Entropy per label

üìä THE MATHEMATICS OF LOSS FUNCTIONS

1. MEAN SQUARED ERROR (MSE) - For Regression:
   L(y, ≈∑) = (1/n) * Œ£(y_i - ≈∑_i)¬≤
   
   Intuition: Penalize large errors heavily
   Gradient: ‚àÇL/‚àÇ≈∑ = 2(≈∑ - y)/n
   
2. MEAN ABSOLUTE ERROR (MAE) - Robust Regression:
   L(y, ≈∑) = (1/n) * Œ£|y_i - ≈∑_i|
   
   Intuition: Linear penalty for errors
   Gradient: ‚àÇL/‚àÇ≈∑ = sign(≈∑ - y)/n
   
3. BINARY CROSS-ENTROPY - For Binary Classification:
   L(y, ≈∑) = -(1/n) * Œ£[y_i * log(≈∑_i) + (1-y_i) * log(1-≈∑_i)]
   
   Intuition: Heavily penalize confident wrong predictions
   Gradient: ‚àÇL/‚àÇ≈∑ = (≈∑ - y) / (≈∑ * (1 - ≈∑))
   
4. CATEGORICAL CROSS-ENTROPY - For Multi-Class:
   L(y, ≈∑) = -(1/n) * Œ£_i Œ£_c y_{i,c} * log(≈∑_{i,c})
   
   Where y is one-hot encoded and ≈∑ is softmax output
   
üî• WHY CROSS-ENTROPY IS MAGICAL

Cross-entropy has special properties:
1. PROBABILISTIC INTERPRETATION: Measures information content
2. CONVEX: Guarantees global minimum (for linear models)
3. GRADIENT PROPERTIES: Clean gradients when combined with softmax
4. INFORMATION THEORY: Measures surprise/uncertainty

The cross-entropy + softmax combination gives:
   ‚àÇL/‚àÇlogits = ≈∑ - y
   
This incredibly clean gradient is why it's the standard choice!

üéØ NUMERICAL STABILITY CONSIDERATIONS

Raw cross-entropy can cause numerical issues:
‚Ä¢ log(0) = -‚àû (when model is confident but wrong)
‚Ä¢ Overflow in softmax with large logits

Solutions:
1. CLIPPING: Ensure probabilities ‚àà [Œµ, 1-Œµ]
2. LOG-SUM-EXP TRICK: Stable softmax computation
3. LOGITS SCALING: Prevent extreme values

‚ö° GRADIENT FLOW PROPERTIES

Good loss functions should:
1. Be DIFFERENTIABLE everywhere we need gradients
2. Have BOUNDED gradients (prevent exploding gradients)
3. Provide INFORMATIVE gradients (non-zero when wrong)
4. Be CONVEX in the parameters (when possible)
""")

class LossFunction:
    """Base class for loss functions."""
    
    def __call__(self, predictions: Tensor, targets: Tensor) -> Tensor:
        raise NotImplementedError
    
    def __repr__(self) -> str:
        return self.__class__.__name__ + "()"

class MeanSquaredError(LossFunction):
    """
    Mean Squared Error loss for regression tasks.
    
    Mathematical Definition:
        L(y, ≈∑) = (1/n) * Œ£(y_i - ≈∑_i)¬≤
        
    Where:
        y = true values
        ≈∑ = predicted values
        n = batch size
        
    Properties:
        - Always non-negative
        - Heavily penalizes large errors (quadratic penalty)
        - Differentiable everywhere
        - Gradient: ‚àÇL/‚àÇ≈∑ = 2(≈∑ - y)/n
        
    Use Cases:
        - Regression problems
        - When you want to penalize large errors heavily
        - When targets are continuous values
        
    Example:
        >>> loss_fn = MeanSquaredError()
        >>> predictions = Tensor([[2.5], [1.8]])
        >>> targets = Tensor([[3.0], [2.0]])
        >>> loss = loss_fn(predictions, targets)
    """
    
    def __call__(self, predictions: Tensor, targets: Tensor) -> Tensor:
        """
        Compute MSE loss.
        
        Args:
            predictions: Model predictions [batch_size, ...]
            targets: Ground truth values [batch_size, ...]
            
        Returns:
            Scalar loss tensor
        """
        if predictions.shape != targets.shape:
            raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
        
        # MSE: mean((predictions - targets)¬≤)
        diff = predictions + (-1) * targets  # predictions - targets
        squared_diff = diff * diff
        loss = squared_diff.mean()
        
        return loss

class MeanAbsoluteError(LossFunction):
    """
    Mean Absolute Error loss for robust regression.
    
    Mathematical Definition:
        L(y, ≈∑) = (1/n) * Œ£|y_i - ≈∑_i|
        
    Properties:
        - Less sensitive to outliers than MSE
        - Linear penalty for errors
        - Not differentiable at zero (but subgradient exists)
        
    Use Cases:
        - Robust regression
        - When outliers are present
        - When you want equal penalty for all error magnitudes
    """
    
    def __call__(self, predictions: Tensor, targets: Tensor) -> Tensor:
        if predictions.shape != targets.shape:
            raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
        
        # MAE: mean(|predictions - targets|)
        diff = predictions + (-1) * targets
        
        # Absolute value using: |x| = sqrt(x¬≤ + Œµ) for numerical stability
        eps = 1e-8
        abs_diff_data = np.sqrt(diff.data ** 2 + eps)
        abs_diff = Tensor(abs_diff_data, requires_grad=diff.requires_grad)
        
        # Connect gradients manually
        if diff.requires_grad:
            abs_diff._children = [diff]
            abs_diff._op = 'abs'
            
            def _backward():
                if diff.requires_grad:
                    diff._ensure_grad()
                    # Gradient of |x| ‚âà x / |x| = sign(x)
                    sign_data = np.sign(diff.data)
                    grad = sign_data * abs_diff.grad.data
                    diff.grad.data += grad
            
            abs_diff._backward_fn = _backward
        
        return abs_diff.mean()

class BinaryCrossEntropy(LossFunction):
    """
    Binary Cross-Entropy loss for binary classification.
    
    Mathematical Definition:
        L(y, p) = -(1/n) * Œ£[y_i * log(p_i) + (1-y_i) * log(1-p_i)]
        
    Where:
        y ‚àà {0, 1} = true binary labels
        p ‚àà (0, 1) = predicted probabilities
        
    Properties:
        - Measures "surprise" when model is wrong
        - Heavily penalizes confident wrong predictions
        - Gradient: ‚àÇL/‚àÇp = (p - y) / (p * (1 - p))
        
    Use Cases:
        - Binary classification
        - Multi-label classification (applied per label)
        - When outputs represent probabilities
        
    Numerical Stability:
        Clips probabilities to [eps, 1-eps] to prevent log(0)
    """
    
    def __init__(self, eps: float = 1e-8):
        """
        Args:
            eps: Small value to prevent log(0)
        """
        self.eps = eps
    
    def __call__(self, predictions: Tensor, targets: Tensor) -> Tensor:
        """
        Compute binary cross-entropy loss.
        
        Args:
            predictions: Predicted probabilities [batch_size, ...]
            targets: Binary targets {0, 1} [batch_size, ...]
            
        Returns:
            Scalar loss tensor
        """
        if predictions.shape != targets.shape:
            raise ValueError(f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}")
        
        # Clip predictions for numerical stability
        pred_clipped_data = np.clip(predictions.data, self.eps, 1 - self.eps)
        pred_clipped = Tensor(pred_clipped_data, requires_grad=predictions.requires_grad)
        
        # Connect gradients
        if predictions.requires_grad:
            pred_clipped._children = [predictions]
            pred_clipped._op = 'clip'
            
            def _clip_backward():
                if predictions.requires_grad:
                    predictions._ensure_grad()
                    # Gradient flows through clipping unchanged where not clipped
                    mask = (predictions.data > self.eps) & (predictions.data < 1 - self.eps)
                    grad = mask.astype(np.float32) * pred_clipped.grad.data
                    predictions.grad.data += grad
            
            pred_clipped._backward_fn = _clip_backward
        
        # BCE: -[y*log(p) + (1-y)*log(1-p)]
        log_pred = Tensor(np.log(pred_clipped_data))
        log_one_minus_pred = Tensor(np.log(1 - pred_clipped_data))
        
        # Manual gradient setup for logarithms
        if pred_clipped.requires_grad:
            log_pred._children = [pred_clipped]
            log_one_minus_pred._children = [pred_clipped]
            log_pred._op = 'log'
            log_one_minus_pred._op = 'log_1minus'
            
            def _log_backward():
                if pred_clipped.requires_grad:
                    pred_clipped._ensure_grad()
                    # ‚àÇlog(p)/‚àÇp = 1/p
                    grad1 = log_pred.grad.data / pred_clipped_data
                    # ‚àÇlog(1-p)/‚àÇp = -1/(1-p)
                    grad2 = -log_one_minus_pred.grad.data / (1 - pred_clipped_data)
                    pred_clipped.grad.data += grad1 + grad2
            
            log_pred._backward_fn = _log_backward
            log_one_minus_pred._backward_fn = _log_backward
        
        # Compute BCE
        term1 = targets * log_pred
        term2 = (1 - targets) * log_one_minus_pred
        loss = -(term1 + term2).mean()
        
        return loss

class CrossEntropyLoss(LossFunction):
    """
    Cross-Entropy loss for multi-class classification.
    
    This combines LogSoftmax + NLLLoss for numerical stability.
    
    Mathematical Definition:
        For logits z and one-hot targets y:
        L = -(1/n) * Œ£_i Œ£_c y_{i,c} * log(softmax(z_i)_c)
        
    Simplified for sparse targets (class indices):
        L = -(1/n) * Œ£_i log(softmax(z_i)_{y_i})
        
    Key Properties:
        - Combines softmax + cross-entropy for stability
        - Works with logits (pre-softmax values)
        - Gradient: ‚àÇL/‚àÇz = (softmax(z) - y) / n
        
    Use Cases:
        - Multi-class classification
        - Language modeling
        - Any task with mutually exclusive classes
    """
    
    def __init__(self, from_logits: bool = True):
        """
        Args:
            from_logits: Whether inputs are logits (True) or probabilities (False)
        """
        self.from_logits = from_logits
    
    def __call__(self, predictions: Tensor, targets: Tensor) -> Tensor:
        """
        Compute cross-entropy loss.
        
        Args:
            predictions: Model outputs [batch_size, num_classes]
                        If from_logits=True: raw logits
                        If from_logits=False: probabilities
            targets: Class indices [batch_size] or one-hot [batch_size, num_classes]
            
        Returns:
            Scalar loss tensor
        """
        if self.from_logits:
            # Apply log-softmax for numerical stability
            logits = predictions
            
            # Stable log-softmax: log(softmax(x)) = x - log(sum(exp(x)))
            # Use log-sum-exp trick: log(sum(exp(x))) = max(x) + log(sum(exp(x - max(x))))
            max_logits_data = np.max(logits.data, axis=-1, keepdims=True)
            max_logits = Tensor(max_logits_data)
            
            shifted_logits = logits + (-1) * max_logits
            exp_shifted = Tensor(np.exp(shifted_logits.data))
            
            sum_exp_data = np.sum(exp_shifted.data, axis=-1, keepdims=True)
            log_sum_exp = Tensor(np.log(sum_exp_data)) + max_logits
            
            log_probs = logits + (-1) * log_sum_exp
        else:
            # Already probabilities, just take log
            pred_clipped_data = np.clip(predictions.data, 1e-8, 1.0)
            log_probs = Tensor(np.log(pred_clipped_data))
        
        # Handle targets
        if len(targets.shape) == 1:
            # Sparse targets (class indices)
            batch_size = targets.shape[0]
            num_classes = predictions.shape[-1]
            
            # Convert to one-hot
            targets_one_hot_data = np.zeros((batch_size, num_classes))
            targets_one_hot_data[np.arange(batch_size), targets.data.astype(int)] = 1
            targets_one_hot = Tensor(targets_one_hot_data)
        else:
            # Already one-hot
            targets_one_hot = targets
        
        # Cross-entropy: -sum(targets * log_probs) / batch_size
        loss = -(targets_one_hot * log_probs).sum() / targets_one_hot.shape[0]
        
        return loss

print("\n‚úÖ LOSS FUNCTIONS IMPLEMENTED!")
print("Available losses: MSE, MAE, BinaryCrossEntropy, CrossEntropyLoss")

# ============================================================================
# PART 6: BUILDING A NEURAL NETWORK - PUTTING IT ALL TOGETHER
# ============================================================================

print("\n\nüíª PART 6: BUILDING COMPLETE NEURAL NETWORKS")
print("="*60)

print("""
üéØ THE MODULAR APPROACH

Now we combine our building blocks into complete neural networks:
‚Ä¢ Linear layers for transformations
‚Ä¢ Activation functions for non-linearity  
‚Ä¢ Loss functions for training objectives
‚Ä¢ A Module system for organization

This is exactly how PyTorch and TensorFlow work!

üèóÔ∏è THE MODULE PATTERN

A Module is a container for:
1. PARAMETERS: Learnable weights and biases
2. FORWARD METHOD: How to compute outputs
3. PARAMETER ACCESS: Easy way to get all parameters
4. GRADIENT MANAGEMENT: Zero gradients, etc.

This allows building complex architectures from simple components.

üîÑ TRAINING LOOP STRUCTURE

The standard deep learning training loop:

1. FORWARD PASS: Compute predictions
   predictions = model(inputs)
   
2. COMPUTE LOSS: Measure error
   loss = loss_function(predictions, targets)
   
3. BACKWARD PASS: Compute gradients
   loss.backward()
   
4. UPDATE PARAMETERS: Improve model
   optimizer.step()
   
5. RESET GRADIENTS: Prepare for next iteration
   model.zero_grad()

üßÆ EXAMPLE ARCHITECTURES

Multi-Layer Perceptron (MLP):
   Input ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí Output
   
Classifier:
   Input ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí Softmax ‚Üí Cross-Entropy Loss
   
Autoencoder:
   Input ‚Üí Encoder ‚Üí Bottleneck ‚Üí Decoder ‚Üí MSE Loss
""")

class Module:
    """
    Base class for all neural network modules.
    
    Inspired by PyTorch's nn.Module, this provides the foundation
    for building modular neural networks.
    
    Key Features:
        - Automatic parameter collection
        - Gradient management
        - Easy composition of layers
        - Training/evaluation modes
        
    All neural network components should inherit from this class.
    """
    
    def __init__(self):
        self.training = True
        self._modules = {}
        self._parameters = {}
    
    def forward(self, *args, **kwargs):
        """Define the forward pass. Must be implemented by subclasses."""
        raise NotImplementedError("Subclasses must implement forward()")
    
    def __call__(self, *args, **kwargs):
        """Make the module callable, delegating to forward()."""
        return self.forward(*args, **kwargs)
    
    def parameters(self) -> List[Tensor]:
        """Return all learnable parameters in this module and its children."""
        params = []
        
        # Add direct parameters
        for param in self._parameters.values():
            if isinstance(param, Tensor):
                params.append(param)
        
        # Add parameters from child modules
        for module in self._modules.values():
            if hasattr(module, 'parameters'):
                params.extend(module.parameters())
        
        return params
    
    def zero_grad(self):
        """Zero gradients of all parameters."""
        for param in self.parameters():
            param.zero_grad()
    
    def train(self, mode: bool = True):
        """Set training mode."""
        self.training = mode
        for module in self._modules.values():
            if hasattr(module, 'train'):
                module.train(mode)
        return self
    
    def eval(self):
        """Set evaluation mode."""
        return self.train(False)
    
    def add_module(self, name: str, module):
        """Add a child module."""
        self._modules[name] = module
        setattr(self, name, module)
    
    def add_parameter(self, name: str, param: Tensor):
        """Add a parameter."""
        self._parameters[name] = param
        setattr(self, name, param)

class Sequential(Module):
    """
    Sequential container for neural network layers.
    
    Passes input through each layer in order, like a pipeline.
    This is the simplest way to build feedforward networks.
    
    Example:
        >>> model = Sequential([
        ...     Linear(784, 128),
        ...     ReLU(),
        ...     Linear(128, 64),
        ...     ReLU(),
        ...     Linear(64, 10)
        ... ])
        >>> output = model(input_tensor)
    """
    
    def __init__(self, layers: List[Module]):
        super().__init__()
        self.layers = layers
        
        # Register layers as modules
        for i, layer in enumerate(layers):
            self.add_module(f'layer_{i}', layer)
    
    def forward(self, x: Tensor) -> Tensor:
        """Pass input through each layer sequentially."""
        for layer in self.layers:
            x = layer(x)
        return x
    
    def __repr__(self) -> str:
        layer_strs = [f"  ({i}): {layer}" for i, layer in enumerate(self.layers)]
        return f"Sequential(\n" + "\n".join(layer_strs) + "\n)"

class MLP(Module):
    """
    Multi-Layer Perceptron (MLP) - a classic feedforward neural network.
    
    Architecture:
        Input ‚Üí Linear ‚Üí Activation ‚Üí ... ‚Üí Linear ‚Üí Output
        
    This is one of the most fundamental neural network architectures,
    consisting of multiple linear layers with non-linear activations.
    
    Args:
        input_size: Number of input features
        hidden_sizes: List of hidden layer sizes
        output_size: Number of output features
        activation: Activation function to use
        output_activation: Final activation (None for linear output)
        dropout_rate: Dropout probability (0 = no dropout)
        
    Example:
        >>> # For MNIST digit classification
        >>> model = MLP(
        ...     input_size=784,      # 28x28 images
        ...     hidden_sizes=[128, 64],
        ...     output_size=10,      # 10 digit classes
        ...     activation=ReLU()
        ... )
    """
    
    def __init__(self, 
                 input_size: int,
                 hidden_sizes: List[int],
                 output_size: int,
                 activation: ActivationFunction = ReLU(),
                 output_activation: Optional[ActivationFunction] = None,
                 dropout_rate: float = 0.0):
        """
        Initialize MLP with specified architecture.
        
        Args:
            input_size: Number of input features
            hidden_sizes: List of hidden layer sizes
            output_size: Number of output features  
            activation: Activation function for hidden layers
            output_activation: Final activation (None for raw outputs)
            dropout_rate: Dropout probability (not implemented yet)
        """
        super().__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.activation = activation
        self.output_activation = output_activation
        
        # Build layers
        layers = []
        
        # Input to first hidden layer
        if hidden_sizes:
            layers.append(Linear(input_size, hidden_sizes[0]))
            layers.append(activation)
            
            # Hidden to hidden layers
            for i in range(len(hidden_sizes) - 1):
                layers.append(Linear(hidden_sizes[i], hidden_sizes[i + 1]))
                layers.append(activation)
                
            # Last hidden to output
            layers.append(Linear(hidden_sizes[-1], output_size))
        else:
            # Direct input to output (no hidden layers)
            layers.append(Linear(input_size, output_size))
        
        # Output activation
        if output_activation is not None:
            layers.append(output_activation)
        
        # Create sequential model
        self.network = Sequential(layers)
        
        # Register as module
        self.add_module('network', self.network)
    
    def forward(self, x: Tensor) -> Tensor:
        """Forward pass through the MLP."""
        return self.network(x)
    
    def __repr__(self) -> str:
        return (f"MLP(input_size={self.input_size}, "
                f"hidden_sizes={self.hidden_sizes}, "
                f"output_size={self.output_size}, "
                f"activation={self.activation})")

print("\n‚úÖ NEURAL NETWORK MODULES IMPLEMENTED!")

# ============================================================================
# PART 7: COMPREHENSIVE TESTING AND EXAMPLES
# ============================================================================

print("\n\nüß™ PART 7: TESTING OUR NEURAL NETWORK FRAMEWORK")
print("="*60)

def test_neural_network_components():
    """Comprehensive test suite for all neural network components."""
    
    print("üß™ Test 1: Weight Initialization")
    print("-" * 30)
    
    # Test different initialization strategies
    layer_shapes = [(100, 50), (784, 128), (512, 256)]
    
    for in_feat, out_feat in layer_shapes:
        print(f"\nLayer shape: ({in_feat}, {out_feat})")
        
        for init_name in ['xavier', 'he', 'lecun']:
            layer = Linear(in_feat, out_feat, weight_init=init_name)
            weights = layer.weight.data
            
            print(f"  {init_name:8}: std={weights.std():.6f}, "
                  f"mean={weights.mean():.6f}")
    
    print("\nüß™ Test 2: Activation Functions")
    print("-" * 30)
    
    # Test activations with sample inputs
    x_test = Tensor([[-2, -1, 0, 1, 2]], requires_grad=True)
    
    activations = {
        'ReLU': ReLU(),
        'GELU': GELU(), 
        'Swish': Swish(),
        'LeakyReLU': LeakyReLU(),
        'Tanh': Tanh(),
        'Sigmoid': Sigmoid()
    }
    
    print(f"Input: {x_test.data}")
    for name, activation in activations.items():
        try:
            output = activation(x_test)
            print(f"{name:10}: {output.data}")
        except Exception as e:
            print(f"{name:10}: Error - {e}")
    
    print("\nüß™ Test 3: Loss Functions")
    print("-" * 30)
    
    # Test regression losses
    pred_reg = Tensor([[2.5, 1.8, 3.2]], requires_grad=True)
    true_reg = Tensor([[3.0, 2.0, 3.0]])
    
    mse = MeanSquaredError()
    mae = MeanAbsoluteError()
    
    mse_loss = mse(pred_reg, true_reg)
    mae_loss = mae(pred_reg, true_reg)
    
    print(f"Regression predictions: {pred_reg.data}")
    print(f"Regression targets:     {true_reg.data}")
    print(f"MSE Loss: {mse_loss.data:.4f}")
    print(f"MAE Loss: {mae_loss.data:.4f}")
    
    # Test classification losses
    pred_binary = Tensor([[0.8, 0.3, 0.9]], requires_grad=True)
    true_binary = Tensor([[1.0, 0.0, 1.0]])
    
    bce = BinaryCrossEntropy()
    bce_loss = bce(pred_binary, true_binary)
    
    print(f"\nBinary predictions: {pred_binary.data}")
    print(f"Binary targets:     {true_binary.data}")
    print(f"BCE Loss: {bce_loss.data:.4f}")
    
    # Test multi-class loss
    logits = Tensor([[2.0, 1.0, 0.5], [1.0, 3.0, 0.5]], requires_grad=True)
    targets = Tensor([0, 1])  # Class indices
    
    ce = CrossEntropyLoss()
    ce_loss = ce(logits, targets)
    
    print(f"\nMulti-class logits: {logits.data}")
    print(f"Multi-class targets: {targets.data}")
    print(f"CrossEntropy Loss: {ce_loss.data:.4f}")
    
    print("\nüß™ Test 4: Complete Neural Network")
    print("-" * 30)
    
    # Create a simple MLP for binary classification
    model = MLP(
        input_size=4,      # 4 features (like Iris dataset)
        hidden_sizes=[8, 4],
        output_size=1,     # Binary classification
        activation=ReLU(),
        output_activation=Sigmoid()
    )
    
    print(f"Model architecture:\n{model}")
    print(f"Total parameters: {len(model.parameters())}")
    
    # Test forward pass
    batch_size = 3
    x = Tensor(np.random.randn(batch_size, 4), requires_grad=True)
    y_true = Tensor([[1], [0], [1]], dtype=np.float32)
    
    print(f"\nInput shape: {x.shape}")
    print(f"Input data:\n{x.data}")
    
    # Forward pass
    y_pred = model(x)
    print(f"Output shape: {y_pred.shape}")
    print(f"Predictions: {y_pred.data.flatten()}")
    
    # Compute loss
    loss_fn = BinaryCrossEntropy()
    loss = loss_fn(y_pred, y_true)
    print(f"Loss: {loss.data:.4f}")
    
    print("\nüß™ Test 5: Gradient Flow")
    print("-" * 30)
    
    # Test that gradients flow correctly through the network
    print("Testing gradient computation...")
    
    # Zero gradients
    model.zero_grad()
    x.zero_grad()
    
    # Backward pass
    loss.backward()
    
    print("‚úÖ Backward pass completed successfully!")
    
    # Check that parameters have gradients
    param_count = 0
    grad_count = 0
    
    for param in model.parameters():
        param_count += 1
        if param.grad is not None:
            grad_count += 1
            print(f"Parameter shape {param.shape}: grad norm = {np.linalg.norm(param.grad.data):.6f}")
    
    print(f"Parameters with gradients: {grad_count}/{param_count}")
    
    if x.grad is not None:
        print(f"Input gradient norm: {np.linalg.norm(x.grad.data):.6f}")
    
    print("\nüß™ Test 6: Training Simulation")
    print("-" * 30)
    
    # Simulate a few training steps
    print("Simulating training steps...")
    
    learning_rate = 0.01
    
    for step in range(5):
        # Generate random batch
        x_batch = Tensor(np.random.randn(batch_size, 4))
        y_batch = Tensor(np.random.randint(0, 2, (batch_size, 1)).astype(np.float32))
        
        # Forward pass
        pred = model(x_batch)
        loss = loss_fn(pred, y_batch)
        
        # Backward pass
        model.zero_grad()
        loss.backward()
        
        # Manual parameter update (simple SGD)
        for param in model.parameters():
            if param.grad is not None:
                param.data -= learning_rate * param.grad.data
        
        print(f"Step {step + 1}: Loss = {loss.data:.4f}")
    
    print("\n‚úÖ ALL TESTS PASSED!")
    print("Your neural network framework is working correctly! üéâ")

# Run the comprehensive tests
test_neural_network_components()

# ============================================================================
# PART 8: REAL-WORLD EXAMPLE - IRIS CLASSIFICATION
# ============================================================================

print("\n\nüå∏ PART 8: REAL-WORLD EXAMPLE - IRIS FLOWER CLASSIFICATION")
print("="*70)

print("""
üéØ BUILDING A COMPLETE CLASSIFIER

Let's build a real classifier for the famous Iris dataset:
‚Ä¢ 4 features: sepal length, sepal width, petal length, petal width
‚Ä¢ 3 classes: Setosa, Versicolor, Virginica
‚Ä¢ 150 samples total

This demonstrates the complete deep learning pipeline:
1. Data preparation
2. Model architecture design
3. Training loop implementation
4. Evaluation metrics

üßÆ ARCHITECTURE CHOICE

For this simple dataset, we'll use:
‚Ä¢ Input: 4 features
‚Ä¢ Hidden: [8, 4] neurons (small network for small dataset)  
‚Ä¢ Output: 3 classes
‚Ä¢ Activation: ReLU (reliable choice)
‚Ä¢ Loss: CrossEntropy (standard for classification)

This is deliberately simple - real datasets need larger networks!
""")

def create_iris_dataset():
    """Create a synthetic Iris-like dataset for demonstration."""
    np.random.seed(42)  # For reproducibility
    
    # Generate synthetic data that mimics Iris characteristics
    n_samples_per_class = 50
    n_features = 4
    
    # Class 0: Setosa (smaller flowers)
    class0 = np.random.normal([5.0, 3.5, 1.5, 0.3], [0.5, 0.4, 0.2, 0.1], 
                             (n_samples_per_class, n_features))
    
    # Class 1: Versicolor (medium flowers)  
    class1 = np.random.normal([6.0, 2.8, 4.5, 1.3], [0.5, 0.4, 0.5, 0.3],
                             (n_samples_per_class, n_features))
    
    # Class 2: Virginica (larger flowers)
    class2 = np.random.normal([6.5, 3.0, 5.5, 2.0], [0.6, 0.4, 0.6, 0.4],
                             (n_samples_per_class, n_features))
    
    # Combine data
    X = np.vstack([class0, class1, class2])
    y = np.hstack([np.zeros(n_samples_per_class),
                   np.ones(n_samples_per_class), 
                   np.full(n_samples_per_class, 2)])
    
    # Shuffle the data
    indices = np.random.permutation(len(X))
    X, y = X[indices], y[indices]
    
    return X.astype(np.float32), y.astype(int)

def train_iris_classifier():
    """Train a neural network on the Iris dataset."""
    
    print("üå± Creating Iris dataset...")
    X, y = create_iris_dataset()
    
    print(f"Dataset shape: {X.shape}")
    print(f"Classes: {np.unique(y)}")
    print(f"Sample features: {X[0]}")
    print(f"Sample label: {y[0]}")
    
    # Split into train/test (simple split)
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    print(f"Training samples: {len(X_train)}")
    print(f"Test samples: {len(X_test)}")
    
    # Create model
    model = MLP(
        input_size=4,
        hidden_sizes=[8, 4],
        output_size=3,  # 3 classes
        activation=ReLU()
        # Note: No output activation - we'll use raw logits with CrossEntropy
    )
    
    print(f"\nüèóÔ∏è Model architecture:\n{model}")
    
    # Loss function
    loss_fn = CrossEntropyLoss(from_logits=True)
    
    # Training parameters
    learning_rate = 0.1
    epochs = 100
    batch_size = 16
    
    print(f"\nüöÇ Training parameters:")
    print(f"Learning rate: {learning_rate}")
    print(f"Epochs: {epochs}")
    print(f"Batch size: {batch_size}")
    
    # Training loop
    print(f"\nüèÉ Starting training...")
    
    train_losses = []
    
    for epoch in range(epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        # Simple batch processing (without proper data loader)
        for i in range(0, len(X_train), batch_size):
            # Get batch
            end_idx = min(i + batch_size, len(X_train))
            X_batch = Tensor(X_train[i:end_idx])
            y_batch = Tensor(y_train[i:end_idx])
            
            # Forward pass
            logits = model(X_batch)
            loss = loss_fn(logits, y_batch)
            
            # Backward pass
            model.zero_grad()
            loss.backward()
            
            # Parameter update (manual SGD)
            for param in model.parameters():
                if param.grad is not None:
                    param.data -= learning_rate * param.grad.data
            
            epoch_loss += loss.data
            num_batches += 1
        
        avg_loss = epoch_loss / num_batches
        train_losses.append(avg_loss)
        
        # Print progress every 20 epochs
        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch + 1:3d}/{epochs}: Loss = {avg_loss:.4f}")
    
    print(f"\n‚úÖ Training completed!")
    print(f"Final training loss: {train_losses[-1]:.4f}")
    
    # Evaluation
    print(f"\nüìä Evaluating on test set...")
    
    # Test the model
    model.eval()  # Set to evaluation mode
    
    X_test_tensor = Tensor(X_test)
    test_logits = model(X_test_tensor)
    
    # Get predictions (argmax of logits)
    test_probs_data = np.exp(test_logits.data)  # Convert logits to probabilities
    test_probs_data = test_probs_data / test_probs_data.sum(axis=1, keepdims=True)  # Normalize
    test_predictions = np.argmax(test_probs_data, axis=1)
    
    # Calculate accuracy
    accuracy = np.mean(test_predictions == y_test)
    print(f"Test accuracy: {accuracy:.2%}")
    
    # Show some predictions
    print(f"\nüîç Sample predictions:")
    print(f"{'True':>4} {'Pred':>4} {'Confidence':>10}")
    print("-" * 20)
    
    for i in range(min(10, len(y_test))):
        true_label = y_test[i]
        pred_label = test_predictions[i]
        confidence = test_probs_data[i, pred_label]
        
        print(f"{true_label:4d} {pred_label:4d} {confidence:9.3f}")
    
    # Class-wise accuracy
    print(f"\nüìà Per-class accuracy:")
    for class_idx in range(3):
        class_mask = y_test == class_idx
        if np.sum(class_mask) > 0:
            class_acc = np.mean(test_predictions[class_mask] == y_test[class_mask])
            print(f"Class {class_idx}: {class_acc:.2%} ({np.sum(class_mask)} samples)")
    
    return model, train_losses, accuracy

# Run the Iris classification example
iris_model, training_losses, final_accuracy = train_iris_classifier()

# ============================================================================
# PART 9: KEY LESSONS AND NEXT STEPS
# ============================================================================

print("\n\nüìö PART 9: KEY LESSONS LEARNED")
print("="*60)

print("""
üéØ WHAT YOU'VE ACCOMPLISHED IN LESSON 3:

1. ‚úÖ WEIGHT INITIALIZATION MASTERY:
   ‚Ä¢ Understood why initialization matters (vanishing/exploding gradients)
   ‚Ä¢ Implemented Xavier, He, and LeCun initialization strategies
   ‚Ä¢ Learned when to use each strategy based on activation functions

2. ‚úÖ LINEAR LAYER IMPLEMENTATION:
   ‚Ä¢ Built the fundamental building block of neural networks
   ‚Ä¢ Correctly implemented forward pass: y = xW + b
   ‚Ä¢ Properly handled gradients with broadcasting
   ‚Ä¢ Created modular, reusable components

3. ‚úÖ ADVANCED ACTIVATION FUNCTIONS:
   ‚Ä¢ Implemented ReLU, GELU, Swish, LeakyReLU, Tanh, Sigmoid
   ‚Ä¢ Understood the mathematical properties of each
   ‚Ä¢ Learned when and why to use different activations
   ‚Ä¢ Solved the "dead ReLU" problem with LeakyReLU

4. ‚úÖ LOSS FUNCTION LIBRARY:
   ‚Ä¢ Implemented MSE and MAE for regression
   ‚Ä¢ Built Binary and Categorical Cross-Entropy for classification
   ‚Ä¢ Handled numerical stability issues (clipping, log-sum-exp trick)
   ‚Ä¢ Understood the information-theoretic basis of cross-entropy

5. ‚úÖ MODULAR NEURAL NETWORK SYSTEM:
   ‚Ä¢ Created a Module base class (like PyTorch's nn.Module)
   ‚Ä¢ Built Sequential for easy model composition
   ‚Ä¢ Implemented MLP for feedforward networks
   ‚Ä¢ Automatic parameter management and gradient handling

6. ‚úÖ COMPLETE TRAINING PIPELINE:
   ‚Ä¢ End-to-end example with real data (Iris classification)
   ‚Ä¢ Manual implementation of training loop
   ‚Ä¢ Model evaluation and accuracy metrics
   ‚Ä¢ Proper train/test split methodology

üß† KEY INSIGHTS GAINED:

1. üéØ MODULARITY IS EVERYTHING:
   Neural networks are just compositions of simple functions.
   The same Linear layer can be used everywhere!

2. üîÑ GRADIENT FLOW IS CRUCIAL:
   Every operation must correctly propagate gradients.
   Broadcasting makes this tricky but manageable.

3. ‚ö° INITIALIZATION SETS THE STAGE:
   Poor initialization = failed training, regardless of architecture.
   He initialization + ReLU is a winning combination.

4. üé® ACTIVATION CHOICE MATTERS:
   ‚Ä¢ ReLU: Fast, reliable, good default
   ‚Ä¢ GELU: Smooth, good for transformers
   ‚Ä¢ Sigmoid/Tanh: Output layers or special cases

5. üìä LOSS FUNCTIONS DEFINE THE TASK:
   ‚Ä¢ MSE: Regression
   ‚Ä¢ Cross-Entropy: Classification
   ‚Ä¢ The loss function determines what the network learns!

6. üèóÔ∏è ARCHITECTURE REFLECTS PROBLEM COMPLEXITY:
   Simple problems (Iris) ‚Üí Small networks
   Complex problems (ImageNet) ‚Üí Deep networks

üöÄ WHAT YOU CAN NOW BUILD:

With your current framework, you can create:
‚úÖ Multi-layer perceptrons for tabular data
‚úÖ Binary and multi-class classifiers  
‚úÖ Regression models for continuous targets
‚úÖ Custom architectures by composing modules
‚úÖ Training loops with proper gradient descent
‚úÖ Model evaluation and metrics

üéØ COMPARISON TO MAJOR FRAMEWORKS:

Your implementation now has core features similar to:

PyTorch:
‚Ä¢ nn.Module base class ‚úÖ
‚Ä¢ nn.Linear layer ‚úÖ  
‚Ä¢ Activation functions ‚úÖ
‚Ä¢ Loss functions ‚úÖ
‚Ä¢ Automatic differentiation ‚úÖ

TensorFlow/Keras:
‚Ä¢ Model composition ‚úÖ
‚Ä¢ Layer abstraction ‚úÖ
‚Ä¢ Training pipeline ‚úÖ
‚Ä¢ Parameter management ‚úÖ

You've built the FOUNDATION of a modern deep learning framework! üéâ

üî• NEXT LESSON PREVIEW: OPTIMIZERS AND ADVANCED TRAINING

Lesson 4 will cover:
‚Ä¢ üéØ Advanced Optimizers (SGD, Adam, AdamW)
‚Ä¢ üìä Learning Rate Scheduling  
‚Ä¢ üé® Regularization Techniques (Dropout, Weight Decay)
‚Ä¢ üìà Training Monitoring and Visualization
‚Ä¢ üîÑ Batch Processing and Data Loading
‚Ä¢ üé≠ Model Checkpointing and Saving
‚Ä¢ üöÄ GPU Acceleration Foundations

üèÜ CHALLENGE EXERCISES:

1. üéØ EASY: Add more activation functions (ELU, Mish)
2. üî• MEDIUM: Implement Dropout layer for regularization
3. üöÄ HARD: Add Batch Normalization layer
4. üí™ EXPERT: Implement a Transformer attention layer
5. üß† RESEARCH: Try different weight initialization schemes

üéâ CONGRATULATIONS! 

You've just built a complete neural network framework from scratch!
This is the foundation that powers all of modern deep learning.

Every time you use PyTorch or TensorFlow, you're using concepts
you just implemented yourself. You now understand what's happening
under the hood of these powerful frameworks.

Ready to tackle optimizers and advanced training in Lesson 4? üöÄ

""")

print("\n" + "="*80)
print("üéâ LESSON 3 COMPLETE: NEURAL NETWORK LAYERS MASTERED!")
print("You now have a complete deep learning framework!")
print("="*80)